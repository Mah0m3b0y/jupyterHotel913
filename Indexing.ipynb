{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a43d4ed-256d-42ef-8127-56706f78aebd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af14d140-a33a-4366-b702-993dec10b8f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %run \"../SAP_FDD_Env_and_Utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a973f3c0-6fdf-4812-93f6-5101d5c6bfbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "787d3ae7-1bc7-4c6b-999a-65c8eff3a769",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ACS_ENDPOINT = \"https://u2zeapebsdse001.search.windows.net\"\n",
    "ACS_KEY = \"vO7O3aspEXtj2qeFTOKl2rLSTNcijSVDN5JNjUV5qxAzSeD0om1d\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89672eae-68ad-4c96-8588-0503b7c8c08c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Function library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f457b8cb-6cac-4b3c-9570-6de3cd281d4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa09f5c-7502-4e6e-a1d4-7048be25f21d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Preparation (reading & chunking)\n",
    "This is used by both the embedded and plain text index. It's crucial that they both have the same document body as source, inluding the same chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69770509-352b-41a3-a103-bdd150891d6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_documents(zip_filename, zip_dir, blacklist_docs=[]):\n",
    "  # load and chunk word documents\n",
    "  print(\"Parsing\")\n",
    "  documents = parse_fsds(blacklist_docs=blacklist_docs, kg_structure_name=zip_filename, kg_directory=zip_dir)\n",
    "  # warnings.filterwarnings(\"ignore\")\n",
    "  return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d72cb1-55aa-4b41-8bf7-1b7c78997f57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_documents_transcripts(doc_path, combine=True, heading=True):\n",
    "  \n",
    "  ## Read the transcripts\n",
    "  transcripts_new = parse_transcripts(doc_path,combine=combine,heading=heading)\n",
    "\n",
    "  ## Quick intermediate step - Write restructured back to JSON (needed for the JSONLoader below)\n",
    "  intermediate_path = \"/tmp/SAP/cleansed_transcript_new_restructured.json\"\n",
    "  with open(intermediate_path, 'w+') as f:\n",
    "      json.dump(transcripts_new, f)\n",
    "\n",
    "  ## Required to retrieve filename as metadata\n",
    "  def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "      metadata[\"transcript_name\"] = record.get(\"filename\")\n",
    "      return metadata\n",
    "\n",
    "  ## Load restructured JSON into langchain (create documents)\n",
    "  loader = JSONLoader(\n",
    "      file_path=intermediate_path,\n",
    "      jq_schema='.[]',\n",
    "      content_key=\"text\",\n",
    "      metadata_func = metadata_func)\n",
    "  texts = loader.load()\n",
    "\n",
    "  return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e66cbc-4f7e-4b8c-8943-1f1f6f4a80ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def chunk_documents(documents, chunk_size=5000, chunk_overlap=1000, heading=True):\n",
    "  print(\"Chunking\")\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap) #TokenTextSplitter\n",
    "  texts = text_splitter.split_documents(documents)\n",
    "  if heading:\n",
    "    for index,chunk in enumerate(texts):\n",
    "      source_filepath=chunk.metadata['source']\n",
    "      source_filename=source_filepath[:source_filepath.rfind(\".\")].split('/')[-1]\n",
    "      texts[index].page_content='Document Name: '+source_filename+'\\n\\n'+chunk.page_content\n",
    "  return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df73c27-f0a0-41c2-ab7a-22bc77f9b565",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Embedded index (FAISS etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce13e2b2-d63c-4051-9f76-c017cc5761aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_embedded_index(documents,index_dir,index_name,embedding_type='OpenAI',index_type='FAISS', save_to_workbench=True):\n",
    "\n",
    "  print(\"Creating embeddings\")\n",
    "  # create embeddings\n",
    "  if embedding_type.replace(' ','').lower()=='huggingface':\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "  elif embedding_type.replace(' ','').lower()=='openai':\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "      deployment=deployment_id_ada,\n",
    "      model=\"text-embedding-ada-002\",\n",
    "      openai_api_key=openai_api_key,\n",
    "      chunk_size=1\n",
    "    )\n",
    "\n",
    "  # create vector store\n",
    "  print(\"Embedding text\")\n",
    "  if index_type.replace(' ','').lower()=='faiss':\n",
    "    db = FAISS.from_documents(documents, embeddings)\n",
    "  # elif index_type.replace(' ','').lower()=='chroma':\n",
    "  #   db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "  if not save_to_workbench or index_dir==None or index_name==None :\n",
    "    return db\n",
    "  \n",
    "  else:\n",
    "    print(\"Saving index to file\")\n",
    "    ## Save in local directory\n",
    "    if not os.path.exists(index_dir):\n",
    "      os.makedirs(index_dir)\n",
    "\n",
    "    with open(os.path.join(index_dir,index_name), \"wb\") as f:\n",
    "      pickle.dump(db,f)\n",
    "    db_ws.CopyFileToWorkbench(filename=index_name, DatabricksFolder=\"file:\"+index_dir)\n",
    "    print('Index'+index_name+' pickled and copied to Workbench')\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35151ec4-3a6f-42a3-89e6-fcb6d90bd9fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Plain text index (Whoosh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c961d0e-9ce1-4e94-9f71-e6127d4663d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DocumentIndexWhoosh():\n",
    "  def from_documents(self, documents, index_path_out, stem=True):\n",
    "\n",
    "    ## Create schema for indexing\n",
    "    stem_ana = StemmingAnalyzer()\n",
    "    schema = Schema(title=ID(stored=True), content=TEXT(stored = True), content_stem=TEXT(analyzer=stem_ana))\n",
    "    \n",
    "    ## Create index\n",
    "    if not os.path.exists(index_path_out):\n",
    "      os.mkdir(index_path_out)\n",
    "    else:\n",
    "      shutil.rmtree(index_path_out)\n",
    "      os.mkdir(index_path_out)\n",
    "    ix = create_in(index_path_out, schema)\n",
    "    writer = ix.writer()\n",
    "\n",
    "    document_dict = []\n",
    "    for document in documents:\n",
    "      document_dict.append({\"text\": document.page_content, \"filename\": document.metadata[\"source\"]})\n",
    "\n",
    "    ## Fill index with documents\n",
    "    for document in document_dict:\n",
    "      writer.add_document(title=document[\"filename\"], content=document[\"text\"], content_stem=document[\"text\"])\n",
    "    writer.commit()\n",
    "    self._index = ix\n",
    "\n",
    "    ## Configure parser to parse search queries\n",
    "    search_field = \"content\"\n",
    "    if stem: search_field = \"content_stem\"\n",
    "    parser = QueryParser(search_field, self._index.schema, group=OrGroup.factory(0.9))\n",
    "    # using OrGroup.factory() with a high value adds a bonus if multiple query words are retrieved (see https://whoosh.readthedocs.io/en/latest/parsing.html)\n",
    "    parser.add_plugin(FuzzyTermPlugin())\n",
    "    self._parser = parser\n",
    "    return self\n",
    "\n",
    "  def load(self, index_path_in):\n",
    "    ix = open_dir(index_path_in)\n",
    "    self._index = ix\n",
    "    search_field = \"content_stem\"\n",
    "    parser = QueryParser(search_field, self._index.schema, group=OrGroup.factory(0.9))\n",
    "    # using OrGroup.factory() with a high value adds a bonus if multiple query words are retrieved (see https://whoosh.readthedocs.io/en/latest/parsing.html)\n",
    "    parser.add_plugin(FuzzyTermPlugin())\n",
    "    self._parser = parser\n",
    "    return self\n",
    "  \n",
    "  def search(self, query, k=5, stem=True, fuzzy_dist=0):\n",
    "    if stem:\n",
    "      ana = StemmingAnalyzer()\n",
    "      query = \" \".join([token.text + \"~\" + str(fuzzy_dist) for token in ana(query)]) #'~1' says that each word can be one character off (fuzzy search)\n",
    "    else:\n",
    "      query = \" \".join([word + \"~\" + str(fuzzy_dist) for word in query.split(\" \")])\n",
    "    try:\n",
    "      # with self._index.searcher() as searcher:\n",
    "      searcher = self._index.searcher()\n",
    "      parsed_query = self._parser.parse(query)\n",
    "      res = searcher.search(parsed_query, terms=True, limit=k)\n",
    "      # Return only the content of the best result (to be able to close the searcher().\n",
    "      # If returning res in full, we'd get an \"ReaderClosed\" exception.)\n",
    "      return res\n",
    "    except Exception as err:\n",
    "      print(\"Error in searching whoosh index. Returning empty string (''). Traceback: \" + str(err))\n",
    "      return \"\"\n",
    "    \n",
    "  def search_and_summarize(self, search_term, k=3, stem=True, fuzzy_dist=0):\n",
    "    res = \"None\"\n",
    "    try:\n",
    "    \n",
    "      # Get initial results from whoosh search\n",
    "      whoosh_results = self.search(search_term, stem=stem, fuzzy_dist=fuzzy_dist)\n",
    "\n",
    "      # avoid index errors (e.g. when we specify top 5, but whoosh only retrieves 3 docs)\n",
    "      if len(whoosh_results) < k:\n",
    "        k = len(whoosh_results)\n",
    "      \n",
    "      # concatenate top k results from the whoosh search in plain text\n",
    "      top_k_docs = [entry[\"content\"] for entry in whoosh_results[:k]]\n",
    "\n",
    "      # have GPT summarize the whoosh-retrieved documents (with a focus on the search keywords)\n",
    "      summary_prompt = f\"\"\"###RequirementStart:\n",
    "      {search_term}\n",
    "      ###RequirementEnd\n",
    "\n",
    "      What detailed granular information from the documents below can be leveraged to build the above requirement? Response should only be each document name followed by relevant info. Elaborate as much as possible. Documents:\n",
    "      '''{top_k_docs}'''\n",
    "      \"\"\"\n",
    "      res = gpt4(summary_prompt, large=True, max_tokens=10000, tries=3, temperature=.2)\n",
    "\n",
    "    except Exception as err:\n",
    "      log(\"[Whoosh Retrieval] Error in searching documents using whoosh & GPT. Returning 'None'. Traceback: \")\n",
    "      print(err)\n",
    "\n",
    "    finally:\n",
    "      return res\n",
    "    \n",
    "## Could also be loaded via ix = index.open_dir(\"indexdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db837308-9e7e-485b-baef-bf39875341c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Combined index (Embedded & plain text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ade9fc-f3c1-4ecb-babe-1fa454c8fadd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CombinedIndex():\n",
    "\n",
    "  def __init__(self, faiss_index, whoosh_index):\n",
    "    self._faiss_index = faiss_index\n",
    "    self._whoosh_index = whoosh_index\n",
    "    self._validate_chunk_equality()\n",
    "\n",
    "  def _validate_chunk_equality(self):\n",
    "    ## Check if individual whoosh and faiss chunks are identical\n",
    "    chunks_whoosh = []\n",
    "    chunks_faiss = []\n",
    "    for doc in self._whoosh_index._index.searcher().documents():\n",
    "      chunks_whoosh.append(doc[\"content\"])\n",
    "    for doc_id in self._faiss_index.docstore._dict.keys():\n",
    "      chunks_faiss.append(self._faiss_index.docstore._dict[doc_id].page_content)\n",
    "    if not len(chunks_whoosh) == len(chunks_faiss): raise ValueError(\"Whoosh and FAISS have do not have the same amount of chunks\")\n",
    "    for i in range(len(chunks_faiss)):\n",
    "      if not chunks_whoosh[i] == chunks_faiss[i]: raise ValueError(\"Whoosh and FAISS chunk text different for chunk number \" + str(i))\n",
    "\n",
    "  def search(self, query, k=5, verbose=False):\n",
    "    whoosh_results_raw = self._whoosh_index.search(query, k=k)\n",
    "    k_whoosh = min(k, len(whoosh_results_raw)) # avoid index errors (e.g. when we specify top 5, but whoosh only retrieves 3 docs)\n",
    "    if verbose and k_whoosh < k: log(f\"[Combined Index Lookup] Whoosh only retrieved k={k_whoosh} results. You specified k={k}.\")\n",
    "    whoosh_results = [entry[\"content\"] for entry in whoosh_results_raw[:k_whoosh]]\n",
    "\n",
    "    faiss_results_raw = self._faiss_index.similarity_search(query, k)\n",
    "    k_faiss= min(k, len(faiss_results_raw)) # avoid index errors (e.g. when we specify top 5, but whoosh only retrieves 3 docs)\n",
    "    if verbose and k_faiss < k: log(f\"[Combined Index Lookup] FAISS only retrieved k={k_faiss} results. You specified k={k}.\")\n",
    "    faiss_results = [entry.page_content for entry in faiss_results_raw[:k_faiss]]\n",
    "\n",
    "    combined_results = [x for x in itertools_chain.from_iterable(zip_longest(whoosh_results,faiss_results)) if x]\n",
    "    # complicated, but needed to preserve scoring order. essentially, this constructes an intertwined list aka [whoosh_1, faiss_1, whoosh_2, faiss_2, ...]\n",
    "    # instead of a naive append ([whoosh_1, whoosh_2, ... , faiss_1, faiss_2] or vice versa).\n",
    "    # note that this prioritizes whoosh over faiss for the first result - change to zip_longest(faiss_results, whoosh_results) to reverse\n",
    "\n",
    "    duplicates = set()\n",
    "    unique_results = []\n",
    "    for x in combined_results:\n",
    "      if x not in unique_results:\n",
    "        unique_results.append(x)\n",
    "      else:\n",
    "        duplicates.add(x)\n",
    "    if verbose and len(duplicates) > 0: log(f\"[Combined Index Lookup] FAISS and Whoosh returned partially same results. Removed {len(duplicates)} duplicates.\")\n",
    "\n",
    "    return unique_results\n",
    "  \n",
    "  def search_and_summarize(self, search_term, k=5, verbose=False):\n",
    "    res = \"None\"\n",
    "    try:\n",
    "    \n",
    "      # Get initial results from whoosh search\n",
    "      top_k_docs = self.search(query=search_term, k=k, verbose=verbose)\n",
    "\n",
    "      # have GPT summarize the whoosh-retrieved documents (with a focus on the search keywords)\n",
    "      summary_prompt = f\"\"\"###RequirementStart:\n",
    "      {search_term}\n",
    "      ###RequirementEnd\n",
    "\n",
    "      What detailed granular information from the documents below can be leveraged to build the above requirement? Response should only be each document name followed by relevant info. Elaborate as much as possible. Documents:\n",
    "      '''{top_k_docs}'''\n",
    "      \"\"\"\n",
    "      res = gpt4(summary_prompt, large=True, max_tokens=5000, tries=3, temperature=.2)\n",
    "\n",
    "    except Exception as err:\n",
    "      log(\"[Combined Index Lookup] Error in searching meeting transcripts using the combined Whoosh & FAISS index. Returning 'None'. Traceback: \")\n",
    "      print(err)\n",
    "\n",
    "    finally:\n",
    "      return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae30bbb-dc9b-4e56-8c47-a5078dddbd71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### ACS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d16a204f-a6a5-4e5e-8450-5e185c48b281",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sh\n",
    "# echo \"\"\"[global]\n",
    "# index-url=https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/\n",
    "# \"\"\" > /etc/pip.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc32a925-cb25-40ba-9dd9-d75576288442",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!pip install --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/ azure-search-documents==11.4.0a20230509004\n",
    "\n",
    "# !pip install azure-search-documents==11.4.0b6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eabba890-eba6-48b7-af1d-316bc24750a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sh\n",
    "# echo \"\" > /etc/pip.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b129b348-a4c7-432e-b94f-f982bfcb3e6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DocumentIndexACS_v2():\n",
    "\n",
    "  def __init__(self, acs_endpoint, acs_key, group_id):\n",
    "    ### Store endpoints, credentials, config variables\n",
    "    self._acs_endpoint = acs_endpoint\n",
    "    self._acs_key = acs_key\n",
    "    self._acs_credential = AzureKeyCredential(self._acs_key)\n",
    "    self.group_id = group_id\n",
    "    self.index_name = \"aifactory-usecases\"\n",
    "\n",
    "  def clear(self):\n",
    "\n",
    "    self.load()\n",
    "\n",
    "    # delete all documents for given group_id\n",
    "    log(f\"[ACS Index] Clearing group_id '{self.group_id}' for index '{self.index_name}' on ACS ...\")\n",
    "    results = list(self._search_client.search(search_text=None, filter=f\"group_ids eq '{self.group_id}'\", top=1000))\n",
    "    if len(results) == 0:\n",
    "      log(f\"[ACS Index] Found no documents for group_id '{self.group_id}' for index '{self.index_name}' on ACS.\")\n",
    "      return\n",
    "    ids = [r['id'] for r in results]\n",
    "    keys =[]\n",
    "    for i in ids:\n",
    "      keys += [{\"id\": i}]\n",
    "      \n",
    "    deleted = self._search_client.delete_documents(keys)\n",
    "    log(f\"[ACS Index] Deleted {len(deleted)} documents for group_id '{self.group_id}' for index '{self.index_name}' on ACS.\")\n",
    "\n",
    "  def from_documents(self, documents, embedding_type='OpenAI'):\n",
    "    log(f\"[ACS Index] Fetching index '{self.index_name}' on Azure Cognitive Search ...\")\n",
    "    self.clear()\n",
    "    self.add_documents(documents, embedding_type)\n",
    "  \n",
    "  def add_documents(self, documents, embedding_type='OpenAI'):\n",
    "\n",
    "    self.load()\n",
    "\n",
    "    ### Embed documents\n",
    "    log(\"[ACS Index] Embedding the documents ...\")\n",
    "    embedded_documents = self._embeddings.embed_documents([doc.page_content for doc in documents])\n",
    "\n",
    "    ### Create structure for ACS upload\n",
    "    log(\"[ACS Index] Generate structure for Azure Cognitive Search upload ...\")\n",
    "    acs_dict = []\n",
    "    for i, doc in enumerate(documents):\n",
    "      acs_dict.append({\n",
    "          'id': str(uuid.uuid1()),\n",
    "          'title': doc.metadata[\"source\"],\n",
    "          'filename': doc.metadata[\"source\"],\n",
    "          'content': doc.page_content,\n",
    "          'content_vector': embedded_documents[i],\n",
    "          \"@search.action\": \"upload\",\n",
    "          \"group_ids\": self.group_id\n",
    "      })\n",
    "\n",
    "    ### Upload to ACS\n",
    "    log(\"[ACS Index] Uploading the embedded documents to Azure Cognitive Search ...\")\n",
    "    search_client = SearchClient(endpoint=self._acs_endpoint,\n",
    "                                  index_name=self.index_name,\n",
    "                                  credential=self._acs_credential)\n",
    "    self._search_client = search_client\n",
    "\n",
    "    result = self._search_client.upload_documents(acs_dict)  \n",
    "    log(f\"[ACS Index] Uploaded {len(acs_dict)} documents\")\n",
    "    return self\n",
    "\n",
    "  def load(self):\n",
    "    search_client = SearchClient(endpoint=self._acs_endpoint,\n",
    "                                 index_name=self.index_name,\n",
    "                                 credential=self._acs_credential)\n",
    "    self._search_client = search_client\n",
    "\n",
    "    # Issue - it's required to know which embeddings have to be used for indexing (so that\n",
    "    # queries can be embedded in the same way). TBD if the search_client obj has that information.\n",
    "    embedding_type = \"openai\" # hardcoding for not - TODO\n",
    "    if embedding_type.replace(' ','').lower()=='huggingface':\n",
    "      embeddings = HuggingFaceEmbeddings()\n",
    "    elif embedding_type.replace(' ','').lower()=='openai':\n",
    "      embeddings = OpenAIEmbeddings(\n",
    "        deployment=deployment_id_ada,\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        openai_api_key=openai_api_key,\n",
    "        chunk_size=1\n",
    "      )\n",
    "    self._embeddings = embeddings # required for search methods later on\n",
    "\n",
    "    # probably not required, just to populate all internal attributes\n",
    "    index_client = SearchIndexClient(endpoint=self._acs_endpoint, credential=self._acs_credential)\n",
    "    self._index_client = index_client\n",
    "\n",
    "    return self\n",
    "  \n",
    "  def search(self, query, k=5, mode=\"text\"):\n",
    "    query_embedded = self._embeddings.embed_query(query)\n",
    "    try:\n",
    "      if mode == \"text\":\n",
    "        results = self._search_client.search(  \n",
    "          search_text=query,\n",
    "          filter=f\"group_ids eq '{self.group_id}'\", #filter query based on group_id\n",
    "          select=[\"title\", \"content\", \"group_ids\"],\n",
    "          top=k\n",
    "        )\n",
    "      elif mode == \"vector\":\n",
    "        query_embedded = self._embeddings.embed_query(query)\n",
    "        results = self._search_client.search(  \n",
    "            search_text=None,\n",
    "            filter=f\"group_ids eq '{self.group_id}'\", #filter query based on group_id\n",
    "            vectors=[Vector(value=query_embedded, k=k, fields=\"content_vector\")], #vector=Vector(value=query_embedded, k=k, fields=\"content_vector\"),  \n",
    "            select=[\"title\", \"content\", \"group_ids\"] \n",
    "        )\n",
    "      elif mode == \"hybrid\":\n",
    "        query_embedded = self._embeddings.embed_query(query)\n",
    "        results = self._search_client.search(  \n",
    "          search_text=query,\n",
    "          filter=f\"group_ids eq '{self.group_id}'\", #filter query based on group_id\n",
    "          vectors=[Vector(value=query_embedded, k=k, fields=\"content_vector\")], #vector=Vector(value=query_embedded, k=k, fields=\"content_vector\"),  \n",
    "          select=[\"title\", \"content\", \"group_ids\"],\n",
    "          top=k\n",
    "        )\n",
    "      else:\n",
    "        print(\"Unsupported search mode. Use either 'text', 'vector', 'hybrid'.\")\n",
    "        return \"\"\n",
    "      content_list_text = [val['content'] for val in results]\n",
    "      return content_list_text\n",
    "    except Exception as err:\n",
    "      print(\"Error in searching ACS index. Returning empty string (''). Traceback: \" + str(err))\n",
    "      raise err #DEBUG\n",
    "      return \"\"\n",
    "    \n",
    "  def search_and_summarize(self, search_term, k=3, mode=\"text\"):\n",
    "    res = \"None\"\n",
    "    try:\n",
    "    \n",
    "      # Get initial results from whoosh search\n",
    "      search_results = self.search(search_term, k=k, mode=mode)\n",
    "\n",
    "      # avoid index errors (e.g. when we specify top 5, but whoosh only retrieves 3 docs)\n",
    "      if len(search_results) < k:\n",
    "        k = len(search_results)\n",
    "      \n",
    "      # concatenate top k results from the whoosh search in plain text\n",
    "      top_k_docs = search_results[:k]\n",
    "\n",
    "      # have GPT summarize the retrieved documents (with a focus on the search keywords)\n",
    "      summary_prompt = f\"Answer the following question given the below documents. The question: {str(search_term)} The documents: {str(top_k_docs)}\"\n",
    "      # res = gpt4(summary_prompt, large=True, max_tokens=10000, tries=3, temperature=.2)\n",
    "      res = gpt35(prompt=summary_prompt, temperature=0.2, max_tokens=4000, tries=3)\n",
    "\n",
    "    except Exception as err:\n",
    "      log(\"[ACS Retrieval] Error in searching documents using ACS & GPT. Returning 'None'. Traceback: \")\n",
    "      print(err)\n",
    "\n",
    "    finally:\n",
    "      return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caedbc27-7eea-496e-ac61-1846605563b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DocumentIndexACS():\n",
    "\n",
    "  def __init__(self, acs_endpoint, acs_key):\n",
    "    ### Store endpoints, credentials, config variables\n",
    "    self._acs_endpoint = acs_endpoint\n",
    "    self._acs_key = acs_key\n",
    "    self._acs_credential = AzureKeyCredential(self._acs_key)\n",
    "\n",
    "  def from_documents(self, documents, index_name, embedding_type='OpenAI'):\n",
    "\n",
    "    ### Create index on ACS endpoint\n",
    "    self.index_name = index_name # for reference\n",
    "    log(\"[ACS Index] Creating the index on Azure Cognitive Search ...\")\n",
    "    index_client = SearchIndexClient(endpoint=self._acs_endpoint, credential=self._acs_credential)\n",
    "    self._index_client = index_client # required for index creation later on\n",
    "\n",
    "    # delete if exists\n",
    "    if index_name in list(self._index_client.list_index_names()):\n",
    "      log(f\"[ACS Index] Dropping existing index '{index_name}' ...\")\n",
    "      self._index_client.delete_index(index_name)\n",
    "    \n",
    "    # define index fields (default)\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String,\n",
    "                        searchable=True),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String,\n",
    "                        searchable=True),\n",
    "        SearchableField(name=\"category\", type=SearchFieldDataType.String,\n",
    "                        filterable=True, searchable=True),\n",
    "        SimpleField(name=\"metadata\", type=SearchFieldDataType.String),\n",
    "        SearchField(name=\"title_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    searchable=True, dimensions=1536, vector_search_configuration=\"my-vector-config\"),\n",
    "        SearchField(name=\"content_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    searchable=True, dimensions=1536, vector_search_configuration=\"my-vector-config\"),\n",
    "    ]\n",
    "\n",
    "    # define vector search config\n",
    "    vector_search = VectorSearch(\n",
    "        algorithm_configurations=[\n",
    "            VectorSearchAlgorithmConfiguration(\n",
    "                name=\"my-vector-config\",\n",
    "                kind=\"hnsw\",\n",
    "                hnsw_parameters={\n",
    "                    \"m\": 10,\n",
    "                    \"efConstruction\": 400,\n",
    "                    \"efSearch\": 500,\n",
    "                    \"metric\": \"cosine\"\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # define semantic search config\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"my-semantic-config\",\n",
    "        prioritized_fields=PrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"title\"),\n",
    "            prioritized_keywords_fields=[SemanticField(field_name=\"category\")],\n",
    "            prioritized_content_fields=[SemanticField(field_name=\"content\")]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the semantic settings with the configuration\n",
    "    semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "    # Create the search index with the semantic settings\n",
    "    index = SearchIndex(name=index_name,\n",
    "                        fields=fields,\n",
    "                        vector_search=vector_search,\n",
    "                        semantic_settings=semantic_settings)\n",
    "    result = self._index_client.create_or_update_index(index)\n",
    "    log(f\"[ACS Index] Index '{result.name}' created on Azure Cognitive Search.\")\n",
    "\n",
    "    ### Create embeddings\n",
    "    if embedding_type.replace(' ','').lower()=='huggingface':\n",
    "      log(f\"[ACS Index] Generating embeddings (HuggingFace) ...\")\n",
    "      embeddings = HuggingFaceEmbeddings()\n",
    "    elif embedding_type.replace(' ','').lower()=='openai':\n",
    "      log(f\"[ACS Index] Generating embeddings (OpenAI ADA2) ...\")\n",
    "      embeddings = OpenAIEmbeddings(\n",
    "        deployment=deployment_id_ada,\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        openai_api_key=openai_api_key,\n",
    "        chunk_size=1\n",
    "      )\n",
    "    self._embeddings = embeddings # required for search methods later on\n",
    "\n",
    "    ### Embed documents\n",
    "    log(\"[ACS Index] Embedding the documents ...\")\n",
    "    embedded_documents = embeddings.embed_documents([doc.page_content for doc in documents])\n",
    "\n",
    "    ### Create structure for ACS upload\n",
    "    log(\"[ACS Index] Generate structure for Azure Cognitive Search upload ...\")\n",
    "    acs_dict = []\n",
    "    for i, doc in enumerate(documents):\n",
    "      acs_dict.append({\n",
    "          'id': str(i), # doc.metadata[\"seq_num\"],\n",
    "          'title': doc.metadata[\"source\"],\n",
    "          'category':'',\n",
    "          'content': doc.page_content,\n",
    "          'content_vector': embedded_documents[i],\n",
    "          \"@search.action\": \"upload\"\n",
    "      })\n",
    "\n",
    "    ### Upload to ACS\n",
    "    log(\"[ACS Index] Uploading the embedded documents to Azure Cognitive Search ...\")\n",
    "    search_client = SearchClient(endpoint=self._acs_endpoint,\n",
    "                                 index_name=self.index_name,\n",
    "                                 credential=self._acs_credential)\n",
    "    self._search_client = search_client\n",
    "\n",
    "    result = self._search_client.upload_documents(acs_dict)  \n",
    "    log(f\"[ACS Index] Uploaded {len(acs_dict)} documents\")\n",
    "    return self\n",
    "\n",
    "  def load(self, index_name):\n",
    "    self.index_name = index_name\n",
    "    search_client = SearchClient(endpoint=self._acs_endpoint,\n",
    "                                 index_name=self.index_name,\n",
    "                                 credential=self._acs_credential)\n",
    "    self._search_client = search_client\n",
    "\n",
    "    # Issue - it's required to know which embeddings have to be used for indexing (so that\n",
    "    # queries can be embedded in the same way). TBD if the search_client obj has that information.\n",
    "    embedding_type = \"openai\" # hardcoding for not - TODO\n",
    "    if embedding_type.replace(' ','').lower()=='huggingface':\n",
    "      embeddings = HuggingFaceEmbeddings()\n",
    "    elif embedding_type.replace(' ','').lower()=='openai':\n",
    "      embeddings = OpenAIEmbeddings(\n",
    "        deployment=deployment_id_ada,\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        openai_api_key=openai_api_key,\n",
    "        chunk_size=1\n",
    "      )\n",
    "    self._embeddings = embeddings # required for search methods later on\n",
    "\n",
    "    # probably not required, just to populate all internal attributes\n",
    "    index_client = SearchIndexClient(endpoint=self._acs_endpoint, credential=self._acs_credential)\n",
    "    self._index_client = index_client\n",
    "\n",
    "    return self\n",
    "  \n",
    "  def search(self, query, k=5, mode=\"text\"):\n",
    "    query_embedded = self._embeddings.embed_query(query)\n",
    "    try:\n",
    "      if mode == \"text\":\n",
    "        results = self._search_client.search(  \n",
    "          search_text=query,   \n",
    "          select=[\"title\", \"content\", \"category\"],\n",
    "          top=k\n",
    "        )\n",
    "      elif mode == \"vector\":\n",
    "        query_embedded = self._embeddings.embed_query(query)\n",
    "        results = self._search_client.search(  \n",
    "            search_text=None,  \n",
    "            vector=Vector(value=query_embedded, k=k, fields=\"content_vector\"),  \n",
    "            select=[\"title\", \"content\", \"category\"] \n",
    "        )\n",
    "      elif mode == \"hybrid\":\n",
    "        query_embedded = self._embeddings.embed_query(query)\n",
    "        results = self._search_client.search(  \n",
    "          search_text=query,  \n",
    "          vector=Vector(value=query_embedded, k=k, fields=\"content_vector\"),  \n",
    "          select=[\"title\", \"content\", \"category\"],\n",
    "          top=k\n",
    "        )\n",
    "      else:\n",
    "        print(\"Unsupported search mode. Use either 'text', 'vector', 'hybrid'.\")\n",
    "        return \"\"\n",
    "      content_list_text = [val['content'] for val in results]\n",
    "      return content_list_text\n",
    "    except Exception as err:\n",
    "      print(\"Error in searching ACS index. Returning empty string (''). Traceback: \" + str(err))\n",
    "      return \"\"\n",
    "    \n",
    "  def search_and_summarize(self, search_term, k=3, mode=\"text\"):\n",
    "    res = \"None\"\n",
    "    try:\n",
    "    \n",
    "      # Get initial results from whoosh search\n",
    "      search_results = self.search(search_term, k=k, mode=mode)\n",
    "\n",
    "      # avoid index errors (e.g. when we specify top 5, but whoosh only retrieves 3 docs)\n",
    "      if len(search_results) < k:\n",
    "        k = len(search_results)\n",
    "      \n",
    "      # concatenate top k results from the whoosh search in plain text\n",
    "      top_k_docs = search_results[:k]\n",
    "\n",
    "      # have GPT summarize the retrieved documents (with a focus on the search keywords)\n",
    "      summary_prompt = f\"\"\"###RequirementStart:\n",
    "      {search_term}\n",
    "      ###RequirementEnd\n",
    "\n",
    "      What detailed granular information from the documents below can be leveraged to build the above requirement? Response should only be each document name followed by relevant info. Elaborate as much as possible. Documents:\n",
    "      '''{top_k_docs}'''\n",
    "      \"\"\"\n",
    "      res = gpt4(summary_prompt, large=True, max_tokens=10000, tries=3, temperature=.2)\n",
    "\n",
    "    except Exception as err:\n",
    "      log(\"[ACS Retrieval] Error in searching documents using ACS & GPT. Returning 'None'. Traceback: \")\n",
    "      print(err)\n",
    "\n",
    "    finally:\n",
    "      return res\n",
    "    \n",
    "## Could also be loaded via ix = index.open_dir(\"indexdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "689778d7-902e-41c5-a5a6-f33bee554f78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Retrieve indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a697bc0f-3c0a-4eb1-b530-0d2b7b1fd488",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_index_func(index_dir,index_name,llm,chain_type=\"stuff\"):\n",
    "\n",
    "  ## Create folder structure if not already present\n",
    "  if not os.path.exists(index_dir):\n",
    "      os.makedirs(index_dir)\n",
    "\n",
    "  db_ws.CopyFileFromWorkbench(\"file:\"+index_dir, filename=index_name)\n",
    "\n",
    "  with open(os.path.join(index_dir,index_name), \"rb\") as f:\n",
    "    db_fdd = pickle.load(f)\n",
    "\n",
    "  index_fdd= RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=db_fdd.as_retriever())\n",
    "  return index_fdd, db_fdd"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "SAP_FDD_Indexing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
