{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b92aa736-a8c2-4736-b202-5cb24a7bbc42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fcd189d-98ff-4bb7-aad9-336b98936f92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "import pickle\n",
    "import ast\n",
    "import tiktoken\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from copy import deepcopy\n",
    "from itertools import zip_longest\n",
    "from itertools import chain as itertools_chain\n",
    "from docx import Document \n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.text.paragraph import Paragraph\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "import unstructured\n",
    "from unstructured import *\n",
    "from unstructured.partition.auto import partition\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "# import beautifulsoup4 as bs4\n",
    "\n",
    "import whoosh\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import QueryParser, OrGroup, FuzzyTermPlugin\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    BaseMessage,\n",
    "    ChatGeneration,\n",
    "    ChatResult,\n",
    ")\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader, CSVLoader, JSONLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import LLMChain, ConversationChain, SimpleSequentialChain, SequentialChain # import LangChain libraries\n",
    "from langchain.llms import OpenAI # import OpenAI model\n",
    "from langchain.prompts import PromptTemplate # import PromptTemplate\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain import VectorDBQA\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "import logging\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import io\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR)\n",
    "\n",
    "from typing import List, Optional, Tuple, Dict, Any\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import sentence_transformers \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.models import Vector  \n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticSettings,  \n",
    "    VectorSearch,  \n",
    "    # VectorSearchAlgorithmConfiguration,  \n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06b419aa-ba91-4424-b392-a9fa1d943824",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2. Databricks & OpenAI Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df6a46b3-9ad7-4b76-93aa-d8c7fb10bee6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">WARNING: v1.3.0 is deprecated. End of support is 04-01-2023. Using databricks_v2 is suggested.\n",
       "Databricks SDK v1.3.0 - successfully loaded\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">WARNING: v1.3.0 is deprecated. End of support is 04-01-2023. Using databricks_v2 is suggested.\nDatabricks SDK v1.3.0 - successfully loaded\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from databricks_sdk_v2.databricks_ws import *\n",
    "from databricks_sdk.databricks_ws import *\n",
    "db_ws = DatabricksWS(dbutils, spark, display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c1a90a2-6d6c-439c-8878-88fd6e48f7fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########################################\n",
    "#           OpenAI API Setup            #\n",
    "#########################################\n",
    "\n",
    "# Get Cluster name and set it to scope variable\n",
    "full_cluster_name = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterName\")\n",
    "\n",
    "cluster_scope = full_cluster_name[:full_cluster_name.rfind(\"_\")]\n",
    "\n",
    "# The OpenAI API key and endpoint URL is currently hosted in an Azure Key Vault\n",
    "api_secret = dbutils.secrets.get(scope = cluster_scope, key = \"OPENAI_API_KEY\")\n",
    "api_url = dbutils.secrets.get(scope = cluster_scope, key = \"OPENAI_URL\")\n",
    "api_header = {\"api-key\": api_secret}\n",
    "\n",
    "### Get deployment\n",
    "# Structuring the Deployment API endpoint\n",
    "deployments_url = f\"{api_url}/openai/deployments?api-version=2023-03-15-preview\"\n",
    "\n",
    "# Call GET List of Deployments\n",
    "deployment_response = requests.get(deployments_url, headers=api_header)\n",
    "\n",
    "# We also have a helper function to search through deployments for a specific model deployment_id\n",
    "def getSpecificModel(deployments, model):\n",
    "    for val in deployments['data']:\n",
    "        if(val['model'] == model):\n",
    "            return val['id']\n",
    "\n",
    "### Get Parameters for different models\n",
    "deployment_id_gpt4 = getSpecificModel(deployment_response.json(), 'gpt-4')\n",
    "deployment_id_gpt35 = getSpecificModel(deployment_response.json(), 'gpt-35-turbo')\n",
    "deployment_id_gpt4_32k = getSpecificModel(deployment_response.json(), 'gpt-4-32k')\n",
    "deployment_id_ada = getSpecificModel(deployment_response.json(), \"text-embedding-ada-002\")\n",
    "deployment_id_gpt35_16k = getSpecificModel(deployment_response.json(), 'gpt-35-turbo-16k')\n",
    "\n",
    "### Structuring the Deployment API endpoint\n",
    "url_gpt4 = f\"{api_url}/openai/deployments/{deployment_id_gpt4}/chat/completions?api-version=2023-03-15-preview\"\n",
    "url_gpt35 = f\"{api_url}/openai/deployments/{deployment_id_gpt35}/chat/completions?api-version=2023-03-15-preview\"\n",
    "url_gpt4_32k = f\"{api_url}/openai/deployments/{deployment_id_gpt4_32k}/chat/completions?api-version=2023-03-15-preview\"\n",
    "url_ada = f\"{api_url}/openai/deployments/{deployment_id_ada}/chat/completions?api-version=2023-03-15-preview\"\n",
    "url_gpt35_16k = f\"{api_url}/openai/deployments/{deployment_id_gpt35}/chat/completions?api-version=2023-03-15-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2739bc3-2174-46c3-b331-a1ec56aee593",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PwCAzureChatOpenAI(AzureChatOpenAI):\n",
    "    def _create_prompt(\n",
    "        self, messages: List[BaseMessage], stop: Optional[List[str]]\n",
    "    ) -> Tuple[str, Dict[str, Any]]:\n",
    "        params: Dict[str, Any] = {\n",
    "            **{\"model\": self.model_name, \"engine\": self.deployment_name},\n",
    "            **self._default_params,\n",
    "        }\n",
    " \n",
    "        params[\"stop\"] = [\"<|im_end|>\"]\n",
    "        prompt = _create_chat_prompt(messages)\n",
    "        return prompt, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4617ba6-36b4-414d-ba7e-06caad45e451",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">WARNING! engine is not default parameter.\n",
       "                    engine was transferred to model_kwargs.\n",
       "                    Please confirm that engine is what you intended.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">WARNING! engine is not default parameter.\n                    engine was transferred to model_kwargs.\n                    Please confirm that engine is what you intended.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt4_32k_model_name = \"gpt-4-32k\"\n",
    "gpt35_16k_model_name = \"gpt-35-turbo-16k\"\n",
    "embed_model_name = \"text-embedding-ada-002\"\n",
    "llm_max_tokens = 20000\n",
    "temperature = 0.0\n",
    "\n",
    "openai_api_base = api_url\n",
    "openai_api_key = api_secret\n",
    "openai_api_type = \"azure\"\n",
    "openai_api_version = \"2023-03-15-preview\"\n",
    "\n",
    "openai.api_base = openai_api_base\n",
    "openai.api_key = openai_api_key\n",
    "openai.api_type = openai_api_type\n",
    "openai.api_version = openai_api_version\n",
    " \n",
    "os.environ[\"OPENAI_API_BASE\"] = openai_api_base\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"OPENAI_API_TYPE\"] = openai_api_type\n",
    "os.environ[\"OPENAI_API_VERSION\"] = openai_api_version\n",
    "\n",
    "llm = PwCAzureChatOpenAI(deployment_name= getSpecificModel(deployment_response.json(), gpt4_32k_model_name),\n",
    "                         model_name=gpt4_32k_model_name,temperature=temperature,max_tokens=llm_max_tokens,engine=gpt4_32k_model_name,\n",
    "                         openai_api_key = openai_api_key,openai_api_base = openai_api_base,openai_api_version = openai_api_version)\n",
    "\n",
    "# llm_35_16k = PwCAzureChatOpenAI(deployment_name = getSpecificModel(deployment_response.json(), gpt35_16k_model_name),\n",
    "#                          model_name=gpt35_16k_model_name,temperature=temperature,max_tokens=llm_max_tokens,engine=gpt35_16k_model_name,\n",
    "#                          openai_api_key = openai_api_key,openai_api_base = openai_api_base,openai_api_version = openai_api_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b92c05d9-355e-4d4b-a5e4-559eee26944a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### ACS Authentication and Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62ceb8b-c0a0-4bde-98d7-46776887e0aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ACS_KEY = \"vO7O3aspEXtj2qeFTOKl2rLSTNcijSVDN5JNjUV5qxAzSeD0om1d\"\n",
    "ACS_NAME = \"u2zeapebsdse001\"\n",
    "ACS_ENDPOINT = f\"https://{ACS_NAME}.search.windows.net\"\n",
    "ACS_CREDENTIAL = AzureKeyCredential(ACS_KEY)\n",
    "\n",
    "os.environ[\"AZURE_COGNITIVE_SEARCH_SERVICE_NAME\"] = ACS_NAME\n",
    "os.environ[\"AZURE_COGNITIVE_SEARCH_API_KEY\"] = ACS_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2fb4a0b-6398-486f-bbc0-1a3d6b6151ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Azure Vision Authentication and Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd17bc61-731f-477b-bafc-6bbdc37b36ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "VISION_ENDPOINT = 'https://testimageanalysis.cognitiveservices.azure.com/'\n",
    "VISION_KEY = 'f920c617e81547c290e94973be55258b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a2319a-7fe6-4ba7-be3f-47a78429fb22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61d3f9d-a6f2-4e89-af29-244cbe4f5cc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def print_red(string, end=\"\\n\"):\n",
    "  print(f\"\\x1b[31m{string}\\x1b[0m\", end=end)\n",
    "def print_green(string, end=\"\\n\"):\n",
    "  print(f\"\\x1b[32m{string}\\x1b[0m\", end=end)\n",
    "def print_blue(string, end=\"\\n\"):\n",
    "  print(f\"\\x1b[36m{string}\\x1b[0m\", end=end)\n",
    "def print_bold(string, end=\"\\n\"):\n",
    "  print(f\"\\x1b[1m{string}\\x1b[0m\", end=end)\n",
    "\n",
    "def log(msg, color=None, end=\"\\n\"):\n",
    "  print_colored = print\n",
    "  if color == \"red\": print_colored = print_red\n",
    "  elif color == \"green\": print_colored = print_green\n",
    "  elif color == \"blue\": print_colored = print_blue\n",
    "  elif color == \"bold\": print_colored = print_bold\n",
    "  print_colored(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\", end=end)\n",
    "\n",
    "# Displays image from local file system (using matplotlib as a workaround, nothing else worked)\n",
    "def print_img(img_path):\n",
    "  image = mpimg.imread(img_path)\n",
    "  plt.imshow(image)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "## Might be useful for GPT - returns an enumerated string for a list, i.e.\n",
    "# 1. X\n",
    "# 2. Y\n",
    "# ...\n",
    "# n. Z\n",
    "def enumerate_str(l):\n",
    "  res_str = \"\"\n",
    "  for index, elem in enumerate(l):\n",
    "    res_str += f\"{index}. {elem} \\n\"\n",
    "  return res_str\n",
    "\n",
    "def create_directories(kg_structure):\n",
    "  try:\n",
    "    Path('/SAP_KG').mkdir(parents=True, exist_ok=True)\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  try:\n",
    "    Path('/SAP_KG/Indexes').mkdir(parents=True, exist_ok=True)\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  for node in kg_structure:\n",
    "    try:\n",
    "      Path('/SAP_KG/' + node).mkdir(parents=True, exist_ok=True)\n",
    "      Path('/SAP_KG/Indexes/' + node).mkdir(parents=True, exist_ok=True)\n",
    "    except:\n",
    "      continue\n",
    "\n",
    "def unzip(file_name, verbose=False):\n",
    "    try:     \n",
    "        os.mkdir('/Team_one_Outputs')\n",
    "        os.mkdir('/Team_one_Outputs/RFP_Samples')\n",
    "        os.mkdir('/Team_one_SummerizeAndSearch')\n",
    "        os.mkdir('/Team_one_SummerizeAndSearchUnzipped')\n",
    "        if verbose == True:\n",
    "          print(file_name+ \"directories successfully created\")\n",
    "    except:\n",
    "        if verbose == True:\n",
    "          print(filename+\" directory creation failed\")\n",
    "        \n",
    "    local_save_folder = 'file:/Team_one_SummerizeAndSearch'\n",
    "    raw_file = db_ws.ListFiles()\n",
    "    raw_file_list=[]\n",
    "    for fil_obj in raw_file:\n",
    "        raw_file_list.append(fil_obj.name)\n",
    "\n",
    "    ws_file = os.path.join(file_name)\n",
    "    db_ws.CopyFileFromWorkbench(DatabricksFolder=local_save_folder,filename=ws_file)\n",
    "\n",
    "    path = '/Team_one_SummerizeAndSearch/'+file_name\n",
    "    try:\n",
    "      with ZipFile(path, 'r') as zObject:\n",
    "          # Extracting all the files of the zip into a temporary folder\n",
    "          zObject.extractall(path=\"/Team_one_SummerizeAndSearchUnzipped\")\n",
    "          list = zObject.namelist()\n",
    "      print(\"all files successfully extracted\")\n",
    "    except:\n",
    "      print(\"file extraction failed\")\n",
    "    return list\n",
    "\n",
    "\n",
    "# def convert(filename,output_path='/Team_one_Outputs/', verbose=False, for_validation = False):\n",
    "def convert(filename,output_path='/tmp/SAP', verbose=False, for_validation = False):\n",
    "    filetype = filename.split('.')[-1]\n",
    "    if filetype not in ['pdf','pptx','doc','docx']:\n",
    "        return ''\n",
    "#   if filetype == 'xlsx':\n",
    "#     convert_xls()\n",
    "#   if filetype == 'csv':\n",
    "#     convert_csv()\n",
    "    if verbose == True:\n",
    "        print(filename+' conversion started...')\n",
    "    if for_validation == True:\n",
    "      elements = partition(filename=\"/tmp/SAP/\"+filename)\n",
    "    else:\n",
    "      elements = partition(filename=\"/Team_one_SummerizeAndSearchUnzipped/\"+filename)\n",
    "    # text=''\n",
    "    # for el in elements:\n",
    "    #   if isinstance(el,unstructured.documents.elements.Title):\n",
    "    #     text=text+'<chunkend>\\n\\n'+str(el)\n",
    "    #   else:\n",
    "    #     text=text+'\\n\\n'+str(el)\n",
    "    # text=text.lstrip('\\n\\n').lstrip('<chunkend>\\n\\n')+'<chunkend>'\n",
    "    text = \"\\n\\n\".join([str(el) for el in elements])\n",
    "    filename = filename[:filename.rfind(\".\")]\n",
    "    filename = filename.split('/')[-1]\n",
    "    if verbose == True:\n",
    "        print(filename+' conversion ended...')\n",
    "    try :\n",
    "        f = open( output_path + filename +'.txt' , \"x\" )\n",
    "    except :\n",
    "        f = open( output_path + filename +'.txt' , \"w+\" )\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def selectFromDict(options, name):\n",
    "  index = 0\n",
    "  indexValidList = []\n",
    "  print('Select a ' + name + ':')\n",
    "  for optionName in options:\n",
    "      index = index + 1\n",
    "      indexValidList.extend([options[optionName]])\n",
    "      print(str(index) + '. ' + optionName)\n",
    "  inputValid = False\n",
    "  while not inputValid:\n",
    "      inputRaw = input(name + ': ')\n",
    "      inputNo = int(inputRaw) - 1\n",
    "      if inputNo > -1 and inputNo < len(indexValidList):\n",
    "          selected = indexValidList[inputNo]\n",
    "          print('Selected ' +  name + ': ' + selected)\n",
    "          inputValid = True\n",
    "          break\n",
    "      else:\n",
    "          print('Please select a valid ' + name + ' number')\n",
    "\n",
    "  return selected\n",
    "\n",
    "def unzip_and_convert(kg_structure,directory,black_list_docs=[],unzip_verbose=False,convert_verbose=False):\n",
    "  for node in kg_structure:\n",
    "    list = unzip(node + '.zip',verbose=unzip_verbose)\n",
    "    for name in list:\n",
    "      if name not in black_list_docs:\n",
    "        text = convert(name, directory + node + \"/\",verbose=convert_verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f368b16-9edb-4139-abae-a3e535f3b2b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 4. Functions For Querying OpenAI / GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "247158bf-a88d-4e31-a963-7e25fb707ea3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########################################\n",
    "#         Core Functions for GPT        #\n",
    "#########################################\n",
    "\n",
    "### Static functions and utils\n",
    "\n",
    "## Query GPT-4\n",
    "def gpt4(prompt:str, context:str = \"You are a helpful assistant.\", temperature:float = 0.0, max_tokens:int = 1000, details:bool = False, large:bool = False, tries:int=1, quiet=True):\n",
    "  \"\"\"\n",
    "  Queries GPT-4 with the specified prompt and parameters.\n",
    "  Example: gpt4(\"Hello\") will return GPT-4's answer: \"Hello! How can I assist you today?\"\n",
    "  \n",
    "  Keyword arguments:\n",
    "  prompt      -- Prompt for GPT-4. Required.\n",
    "  context     -- System context for GPT-4. Optional, default 'You are a helpful assistant.'.\n",
    "  temperature -- Temperature parameter for GPT-4. Optional, default 0.\n",
    "  max_tokens  -- Max Token Parameter for GPT-4. Optional, default 1000.\n",
    "  details     -- Whether or not the full API response (incl. headers etc.) or just the model's answer will be returned. Optional, default False.\n",
    "\n",
    "  Output: The response from the GPT-4 API. Depending on the 'details' parameter, either the full API response (list/dict) or just GPT's answer (string).\n",
    "  \"\"\"\n",
    "  # url = url_gpt4_32k if large else url_gpt4\n",
    "  url = url_gpt4\n",
    "  headers = api_header\n",
    "  data = {\n",
    "    \"messages\":[\n",
    "      {\"role\": \"system\", \"content\": context},\n",
    "      {\"role\": \"user\", \"content\": prompt+\"\\n\"}\n",
    "    ],\n",
    "    \"temperature\": temperature, \n",
    "    \"max_tokens\": max_tokens\n",
    "  }\n",
    "  # Call POST to run Completion on a deployment\n",
    "  for i in range(tries):\n",
    "    response = None\n",
    "    try:\n",
    "      response = requests.post(url, headers=headers, json=data)\n",
    "      print(response.json())\n",
    "    except Exception as err:\n",
    "      if not quiet: print_bold(\"ERROR - Error in GPT response. No response generated. Retrying in 60 seconds.\")\n",
    "      if not quiet: print(err)\n",
    "      if i < tries-1:\n",
    "        time.sleep(60)\n",
    "      continue\n",
    "    try:\n",
    "      return response.json()['choices'][0]['message']['content'] if details == False else response.json()\n",
    "    except Exception as err:\n",
    "      sleeptime = 15 # default, wait 15 seconds before retry\n",
    "      if not quiet: print_bold(\"ERROR - Error in GPT response. Full response:\")\n",
    "      if not quiet: print(response.text)\n",
    "      if i < tries-1:\n",
    "        search = re.search(r\"Please retry after ([\\d]*) seconds\", response.text)\n",
    "        if search: sleeptime = int(search.group(1))\n",
    "        if not quiet: print(f\"Trying again in {sleeptime} seconds.\")\n",
    "        time.sleep(sleeptime)\n",
    "      continue\n",
    "\n",
    "## Query GPT-3.5\n",
    "def gpt35(prompt:str, context:str = \"You are a helpful assistant.\", temperature:float = 0.0, max_tokens:int = 1000, details:bool = False, large= False, tries:int=1):\n",
    "  \"\"\"\n",
    "  Queries GPT-3.5 (Turbo) with the specified prompt and parameters.\n",
    "  Example: gpt35(\"Hello\") will return GPT-3.5's answer: \"Hello! How can I assist you today?\"\n",
    "  \n",
    "  Keyword arguments:\n",
    "  prompt      -- Prompt for GPT-3.5. Required.\n",
    "  context     -- System context for GPT-3.5. Optional, default 'You are a helpful assistant.'.\n",
    "  temperature -- Temperature parameter for GPT-3.5. Optional, default 0.\n",
    "  max_tokens  -- Max Token Parameter for GPT-3.5. Optional, default 1000.\n",
    "  details     -- Whether or not the full API response (incl. headers etc.) or just the model's answer will be returned. Optional, default False.\n",
    "\n",
    "  Output: The response from the GPT-3.5 API. Depending on the 'details' parameter, either the full API response (list/dict) or just GPT's answer (string).\n",
    "  \"\"\"\n",
    "  # url = url_gpt35_16k if large else url_gpt35\n",
    "  url = url_gpt35_16k\n",
    "  headers = api_header\n",
    "  data = {\n",
    "    \"messages\":[\n",
    "      {\"role\": \"system\", \"content\": context},\n",
    "      {\"role\": \"user\", \"content\": prompt+\"\\n\"}\n",
    "    ],\n",
    "    \"temperature\": temperature, \n",
    "    \"max_tokens\": max_tokens\n",
    "  }\n",
    "  # Call POST to run Completion on a deployment\n",
    "  for i in range(tries):\n",
    "    response = None\n",
    "    try:\n",
    "      response = requests.post(url, headers=headers, json=data)\n",
    "    except Exception as err:\n",
    "      print(\"ERROR - Error in GPT response. No response generated. Retrying in 60 seconds.\")\n",
    "      print(err)\n",
    "      if i < tries-1:\n",
    "        time.sleep(60)\n",
    "      continue\n",
    "    try:\n",
    "      print(response.json())\n",
    "      return response.json()['choices'][0]['message']['content'] if details == False else response.json()\n",
    "    except Exception as err:\n",
    "      sleeptime = 15 # default, wait 15 seconds before retry\n",
    "      print(\"ERROR - Error in GPT response. Full response:\")\n",
    "      print(response.text)\n",
    "      if i < tries-1:\n",
    "        search = re.search(r\"Please retry after ([\\d]*) seconds\", response.text)\n",
    "        if search: sleeptime = int(search.group(1))\n",
    "        print(f\"Trying again in {sleeptime} seconds.\")\n",
    "        time.sleep(sleeptime)\n",
    "      continue\n",
    "\n",
    "### Prototype - Class to enable GPT-4 prompts with memory of previous prompts\n",
    "class gpt4_withMemory:\n",
    "  def __init__(self, context:str = \"You are a helpful assistant.\"):\n",
    "    self.context = context\n",
    "    self.memory = [\n",
    "      {\"role\": \"system\", \"content\": self.context}\n",
    "    ]\n",
    "  def prompt(self, prompt:str, temperature:float = 0.0, max_tokens:int = 300, details:bool = False, large = False):\n",
    "    self.memory.append({\"role\": \"user\", \"content\": prompt+\"\\n\"})\n",
    "    url = url_gpt4_32k if large else url_gpt4\n",
    "    headers = api_header\n",
    "    data = {\n",
    "      \"messages\": self.memory,\n",
    "      \"temperature\": temperature, \n",
    "      \"max_tokens\": max_tokens\n",
    "    }\n",
    "    # Call POST to run Completion on a deployment\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    self.memory.append({\"role\": \"assistant\", \"content\": response.json()['choices'][0]['message']['content']})\n",
    "    return response.json()['choices'][0]['message']['content'] if details == False else response.json()\n",
    "  \n",
    "  def prompt_with_context(self, prompt:str, context, temperature:float = 0.0, max_tokens:int = 1000, details:bool = False, large = False):\n",
    "    docs = context.similarity_search(prompt)\n",
    "    full_prompt = f\"\"\"\n",
    "      Given the following extracted parts of a long maintenance manual and a question, create a final answer.\n",
    "      Maintenance Manual:{docs}\n",
    "      Question: {prompt}\n",
    "      \"\"\"\n",
    "    return self.prompt(prompt=full_prompt, temperature=temperature, max_tokens=max_tokens, details=details, large=large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adde481a-a48c-4b5f-85bc-0f2b4427a88d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 5. Functions For Manipulating Word Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "696804f4-e4b9-41ae-8a00-6371f67d3ac8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########################################\n",
    "#     Core Functions for Word/docx      #\n",
    "#########################################\n",
    "### import unicodedata\n",
    "### txt = \" weird word text\"\n",
    "### cleaned_up_text = unicodedata.normalize(\"NFKD\",text)\n",
    "# Helper function for retrieving the token count of a text\n",
    "def count_tokens(text, model_name=\"gpt-4\"):\n",
    "  encoding = tiktoken.encoding_for_model(model_name)\n",
    "  num_tokens = len(encoding.encode(text))\n",
    "  return num_tokens\n",
    "    \n",
    "# Get section headers for tables recursively\n",
    "def get_table_title(table):\n",
    "    return Paragraph(table._element.getprevious(), table._element.getprevious().getparent()).text\n",
    "\n",
    "# Slices a list l into n chunks (sequentially). E.g. [1, 2, 3, 4, 5] would become [[1, 2], [3, 4], [5]] for n=3.\n",
    "def slice_list(l, n):\n",
    "    d, r = divmod(len(l), n)\n",
    "    for i in range(n):\n",
    "        si = (d+1)*(i if i < r else r) + d*(0 if i < r else i - r)\n",
    "        yield l[si:si+(d+1 if i < r else d)]\n",
    "\n",
    "# Running paragraphs content between '<' and '>'\n",
    "def runningparagraphfunc(text,runningparagraph,unclosedstarttext,startingindex,index):\n",
    "  unclosedplaceholderlist=re.findall(r'<[^>]*$',text)\n",
    "  closedplaceholderlist=re.findall(r'^[^<]*>',text)\n",
    "  if len(unclosedplaceholderlist)>0:\n",
    "      unclosedstarttext=unclosedplaceholderlist[0]\n",
    "      runningparagraph=unclosedstarttext\n",
    "      startingindex=index\n",
    "\n",
    "  if len(runningparagraph)>0:\n",
    "      if len(closedplaceholderlist)==0 and len(unclosedplaceholderlist)==0:\n",
    "          runningparagraph=(runningparagraph+'\\n'+text).strip('\\n')\n",
    "      elif len(closedplaceholderlist)>0 and len(unclosedplaceholderlist)==0:\n",
    "          runningparagraph=(runningparagraph+'\\n'+closedplaceholderlist[0]).strip('\\n')\n",
    "  return startingindex,unclosedstarttext,runningparagraph\n",
    "\n",
    "# Read the Word documentd\n",
    "def read_document(file_path, max_tokens_per_chunk=3000):\n",
    "    doc = Document(file_path)\n",
    "    originalpositions = {\"paragraphs\": [], \"tables\": []}\n",
    "    placeholders = {\"paragraphs\": [], \"tables\": []}\n",
    "    current_section = None\n",
    "    table_current_section = None\n",
    "    headinglist=[]\n",
    "    runningparagraph=\"\"\n",
    "    unclosedstarttext=\"\"\n",
    "    startingindex=0\n",
    "\n",
    "    for index, paragraph in enumerate(doc.paragraphs):\n",
    "        if paragraph.style.name.startswith('Heading'):\n",
    "            headinglist.append(paragraph.text)\n",
    "            current_section = paragraph.text\n",
    "            runningparagraph=\"\"\n",
    "            unclosedstarttext=\"\"\n",
    "            startingindex=0\n",
    "        \n",
    "        placeholder_matches=re.findall(r'<[^>]*>', paragraph.text)\n",
    "        if placeholder_matches:\n",
    "            for placeholder in placeholder_matches:\n",
    "                if [obj for obj in placeholders[\"paragraphs\"] if obj['section_header']==current_section and obj['placeholder_text']==placeholder]==[]:\n",
    "                    placeholders[\"paragraphs\"].append({\n",
    "                        \"section_header\": current_section,\n",
    "                        \"placeholder_text\": placeholder,\n",
    "                        \"replacement_text\": \"\"})\n",
    "                originalpositions[\"paragraphs\"].append({\n",
    "                    \"section_header\": current_section,\n",
    "                    \"placeholder_text\": placeholder,\n",
    "                    \"paragraph_id\": index,\n",
    "                    \"original_text\": placeholder,\n",
    "                    \"keep_original_text\": False})\n",
    "        \n",
    "        startingindex,unclosedstarttext,runningparagraph=runningparagraphfunc(paragraph.text,runningparagraph,unclosedstarttext,startingindex,index)\n",
    "        \n",
    "        if len(re.findall(r'^[^<]*>',paragraph.text))>0:\n",
    "            if [obj for obj in placeholders[\"paragraphs\"] if obj['section_header']==current_section and obj['placeholder_text']==runningparagraph]==[]:\n",
    "                placeholders[\"paragraphs\"].append({\n",
    "                        \"section_header\": current_section,\n",
    "                        \"placeholder_text\": runningparagraph,\n",
    "                        \"replacement_text\": \"\"})\n",
    "            originalpositions[\"paragraphs\"].append({\n",
    "                    \"section_header\": current_section,\n",
    "                    \"placeholder_text\": runningparagraph,\n",
    "                    \"paragraph_id\": startingindex,\n",
    "                    \"original_text\": unclosedstarttext,\n",
    "                    \"keep_original_text\": True})\n",
    "\n",
    "            runningparagraph=\"\"\n",
    "\n",
    "    cellheadermetadata=pd.DataFrame()\n",
    "    cellheaderlist=[]\n",
    "    tableindexlist=[]\n",
    "    rowindexlist=[]\n",
    "    cellindexlist=[]\n",
    "    prevsectionheading=''\n",
    "\n",
    "    #Table cell headers\n",
    "    for table_index, table in enumerate(doc.tables):\n",
    "        for row_index, row in enumerate(table.rows):\n",
    "            for cell_index, cell in enumerate(row.cells):\n",
    "                cellallcontent=''\n",
    "                for cell_paragraph in cell.paragraphs:\n",
    "                    cellallcontent=(cellallcontent+'\\n'+cell_paragraph.text).strip('\\n')\n",
    "                if re.findall(r'<[^>]*>', cellallcontent)==[]:\n",
    "                    cellheaderlist.append(cellallcontent)\n",
    "                    tableindexlist.append(table_index)\n",
    "                    rowindexlist.append(row_index)\n",
    "                    cellindexlist.append(cell_index)\n",
    "    cellheadermetadata['table index']=pd.Series(tableindexlist)\n",
    "    cellheadermetadata['row index']=pd.Series(rowindexlist)\n",
    "    cellheadermetadata['cell index']=pd.Series(cellindexlist)\n",
    "    cellheadermetadata['cell header metadata']=pd.Series(cellheaderlist)\n",
    "\n",
    "    # cumulative concat headers, not considered for time being\n",
    "    # cellheadermetadata['column header metadata']=cellheadermetadata.groupby(['table index','cell index'])['cell header metadata'].transform(lambda g: g.add('>').cumsum()).str.rstrip('>')\n",
    "    # cellheadermetadata['row header metadata']==cellheadermetadata.groupby(['table index','row index'])['cell header metadata'].transform(lambda g: g.add('>').cumsum()).str.rstrip('>')\n",
    "\n",
    "    for table_index, table in enumerate(doc.tables):\n",
    "        sectionheading=''\n",
    "        inputelement=table._element\n",
    "        while sectionheading not in headinglist:\n",
    "          try:\n",
    "            inputelement=inputelement.getprevious()\n",
    "            sectionheading=Paragraph(inputelement, inputelement.getparent()).text\n",
    "          except:\n",
    "            sectionheading=prevsectionheading\n",
    "        prevsectionheading=sectionheading\n",
    "        for row_index, row in enumerate(table.rows):\n",
    "            for cell_index, cell in enumerate(row.cells):\n",
    "                runningparagraph=\"\"\n",
    "                unclosedstarttext=\"\"\n",
    "                startingindex=0\n",
    "                cellallcontent=''\n",
    "                for cell_paragraph in cell.paragraphs:\n",
    "                    #calculate last column and row headers for cell\n",
    "                    try:\n",
    "                      columnheader=cellheadermetadata.loc[(cellheadermetadata['row index']<row_index)&(cellheadermetadata['cell index']==cell_index)&(cellheadermetadata['table index']==table_index),'cell header metadata'].tail(1).values[0]\n",
    "                    except:\n",
    "                      columnheader=''\n",
    "                    try:\n",
    "                      rowheader=cellheadermetadata.loc[(cellheadermetadata['cell index']<cell_index)&(cellheadermetadata['row index']==row_index)&(cellheadermetadata['table index']==table_index),'cell header metadata'].tail(1).values[0]\n",
    "                    except:\n",
    "                      rowheader=''\n",
    "                    #generate final section header value\n",
    "                    if columnheader.lower().replace(' ','')!=sectionheading.lower().replace(' ',''):\n",
    "                      columnheaderval=columnheader.strip()\n",
    "                    else:\n",
    "                      columnheaderval=''\n",
    "                    if rowheader.lower().replace(' ','')!=sectionheading.lower().replace(' ','') and rowheader.lower().replace(' ','')!=columnheader.lower().replace(' ',''):\n",
    "                      rowheaderval=rowheader.strip()\n",
    "                    else:\n",
    "                      rowheaderval=''\n",
    "                    final_section_header=(sectionheading.strip()+'>'+(columnheaderval+'>'+rowheaderval).strip('>')).strip('>')\n",
    "                    #check for placeholders to be replaced\n",
    "                    placeholder_matches = re.findall(r'<[^>]*>', cell_paragraph.text)\n",
    "                    if placeholder_matches:\n",
    "                        for placeholder in placeholder_matches:\n",
    "                            if [obj for obj in placeholders[\"tables\"] if obj['section_header']==final_section_header and obj['placeholder_text']==placeholder]==[]:\n",
    "                                placeholders[\"tables\"].append({\n",
    "                                  \"section_header\": final_section_header,\n",
    "                                  \"placeholder_text\": placeholder,\n",
    "                                  \"replacement_text\": \"\"})\n",
    "                            originalpositions[\"tables\"].append({\n",
    "                                \"section_header\": final_section_header,\n",
    "                                \"placeholder_text\": placeholder,\n",
    "                                \"location\": {\n",
    "                                    \"table_id\": table_index, \"row_id\": row_index, \"cell_id\": cell_index\n",
    "                                    },\n",
    "                                \"original_text\": placeholder,\n",
    "                                \"keep_original_text\": False})\n",
    "\n",
    "                    startingindex,unclosedstarttext,runningparagraph=runningparagraphfunc(cell_paragraph.text,runningparagraph,unclosedstarttext,startingindex,index=0)\n",
    "            \n",
    "                    if len(re.findall(r'^[^<]*>',cell_paragraph.text))>0:\n",
    "                        if [obj for obj in placeholders[\"tables\"] if obj['section_header']==final_section_header and obj['placeholder_text']==runningparagraph]==[]:\n",
    "                            placeholders[\"tables\"].append({\n",
    "                                    \"section_header\": final_section_header,\n",
    "                                    \"placeholder_text\": runningparagraph,\n",
    "                                    \"replacement_text\": \"\"})\n",
    "                        originalpositions[\"tables\"].append({\n",
    "                                \"section_header\": final_section_header,\n",
    "                                \"placeholder_text\": runningparagraph,\n",
    "                                \"location\": {\n",
    "                                    \"table_id\": table_index, \"row_id\": row_index, \"cell_id\": cell_index\n",
    "                                    },\n",
    "                                \"original_text\": unclosedstarttext,\n",
    "                                \"keep_original_text\": True})\n",
    "                        runningparagraph=\"\"\n",
    "\n",
    "    token_count_paragraphs = count_tokens(json.dumps(placeholders[\"paragraphs\"]))\n",
    "    token_count_tables = count_tokens(json.dumps(placeholders[\"tables\"]))\n",
    "    # print(\"Token count for Paragraphs placeholder dict:\", token_count_paragraphs)\n",
    "    # print(\"Token count for Tables placeholder dict:\", token_count_tables)\n",
    "\n",
    "    if token_count_paragraphs > max_tokens_per_chunk:\n",
    "      n = ceil(token_count_paragraphs / max_tokens_per_chunk)\n",
    "      print(f\"Warning: Token count for paragraphs is {token_count_paragraphs} and thus higher than the specified max ({max_tokens_per_chunk}). Splitting paragraphs into {n} chunks, retrieve them by using placeholders['paragraphs']['chunks'][n] or increase the max_tokens_per_chunk parameter.\")\n",
    "      chunks = list(slice_list(placeholders[\"paragraphs\"], n))\n",
    "      placeholders[\"paragraphs\"] = {\"chunks\": chunks}\n",
    "    else:\n",
    "      placeholders[\"paragraphs\"] = {\"chunks\": [placeholders[\"paragraphs\"]]}\n",
    "\n",
    "    if token_count_tables > max_tokens_per_chunk:\n",
    "      n = ceil(token_count_tables / max_tokens_per_chunk)\n",
    "      print(f\"Warning: Token count for tables is {token_count_tables} and thus higher than the specified max ({max_tokens_per_chunk}). Splitting tables into {n} chunks, retrieve them by using placeholders['tables']['chunks'][n] or increase the max_tokens_per_chunk parameter.\")\n",
    "      chunks = list(slice_list(placeholders[\"tables\"], n))\n",
    "      placeholders[\"tables\"] = {\"chunks\": chunks}\n",
    "    else:\n",
    "      placeholders[\"tables\"] = {\"chunks\": [placeholders[\"tables\"]]}\n",
    "\n",
    "    return doc, placeholders, originalpositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c05fd725-a37b-4333-a9fe-653060123af8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########################################\n",
    "#     Core Functions for Word/docx      #\n",
    "#########################################\n",
    "\n",
    "def write_back_to_document(documentinput, placeholders, originalpositions, verbose=False):\n",
    "  \n",
    "  #document=deepcopy(documentinput) DOES NOT WORK\n",
    "  # with deepcopy(), document.save() will save the previous version (i.e. nothing replaced -.-)\n",
    "  # This seems to be an undocumented Bug in pythondocx ...\n",
    "  # Workaround:\n",
    "  documentinput.save(\"tmp_clone.docx\")\n",
    "  document = Document(\"tmp_clone.docx\")\n",
    "  \n",
    "  def replace_in_paragraph(document, paragraph_id, placeholder_text, replacement_text, verbose):\n",
    "    paragraph = document.paragraphs[paragraph_id]\n",
    "    paragraph.text = paragraph.text.replace(placeholder_text, replacement_text)\n",
    "    if verbose: print(f\"[Paragraph {paragraph_id}] Replaced '{placeholder_text}' with '{replacement_text}'\")\n",
    "\n",
    "  def replace_in_table(document, table_id, row_id, cell_id, placeholder_text, replacement_text, verbose):\n",
    "    cell = document.tables[table_id].rows[row_id].cells[cell_id]\n",
    "    cell.text = cell.text.replace(placeholder_text, replacement_text)\n",
    "    if verbose: print(f\"[Table {table_id} - Row {row_id} Column {cell_id}] Replaced '{placeholder_text}' with '{replacement_text}'\")\n",
    "\n",
    "  for chunk in placeholders[\"paragraphs\"][\"chunks\"]:\n",
    "    for chunkindex,chunkelem in enumerate(chunk):\n",
    "      for index,elem in enumerate([obj for obj in originalpositions[\"paragraphs\"] if obj['section_header']==chunkelem[\"section_header\"] and obj['placeholder_text']==chunkelem[\"placeholder_text\"]]):\n",
    "        if elem['keep_original_text']==False:\n",
    "          replacement_text=chunkelem['replacement_text']\n",
    "        else:\n",
    "          replacement_text=chunkelem['replacement_text']+'\\n\\n'+elem[\"original_text\"]\n",
    "        replace_in_paragraph(document, elem[\"paragraph_id\"], elem[\"original_text\"], replacement_text, verbose=verbose)\n",
    "\n",
    "  for chunk in placeholders[\"tables\"][\"chunks\"]:\n",
    "    for chunkindex,chunkelem in enumerate(chunk):\n",
    "      for index,elem in enumerate([obj for obj in originalpositions[\"tables\"] if obj['section_header']==chunkelem[\"section_header\"] and obj['placeholder_text']==chunkelem[\"placeholder_text\"]]):\n",
    "        if elem['keep_original_text']==False:\n",
    "          replacement_text=chunkelem['replacement_text']\n",
    "        else:\n",
    "          replacement_text=chunkelem['replacement_text']+'\\n\\n'+elem[\"original_text\"]\n",
    "        replace_in_table(document, elem[\"location\"][\"table_id\"],elem[\"location\"][\"row_id\"], elem[\"location\"][\"cell_id\"], elem[\"original_text\"], replacement_text, verbose=verbose)\n",
    "      \n",
    "  return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8dd7c1-4264-4811-b02b-483a49f0f53c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create list of headings from a filepath, used primarilty in validation testing\n",
    "\n",
    "def create_heading_list(filepath):\n",
    "  doc = Document(filepath)\n",
    "  headinglist=[]\n",
    "\n",
    "  for index, paragraph in enumerate(doc.paragraphs):\n",
    "    if paragraph.style.name.startswith('Heading') and len(paragraph.text.strip())>0:\n",
    "       headinglist.append(paragraph.text)\n",
    "  return headinglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60d156f-5493-4196-ab45-9b066028861f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## split text to df based on headers, primarily used in validation\n",
    "\n",
    "def split_text_to_df(text, header_titles):\n",
    "    sections = text.split(\"\\n\\n\")\n",
    "    headers = []\n",
    "    section_texts = []\n",
    "    current_text= \"\"\n",
    "    counter=0\n",
    "    for item in sections:\n",
    "        if item in header_titles and item != \"\":\n",
    "            counter=counter+1\n",
    "            headers.append(item)\n",
    "            if counter>1:\n",
    "                section_texts.append(current_text)\n",
    "            current_text= \"\"\n",
    "        elif item not in header_titles and item != \"\":\n",
    "            current_text=(current_text+\"\\n\\n\"+item).strip(\"\\n\\n\")\n",
    "    section_texts.append(current_text)\n",
    "    df = pd.DataFrame({'Headers': headers, 'Sections': section_texts})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f33a2dc4-3cbc-473c-8575-994ddee01ac7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_dups_from_text(text1, text2):\n",
    "  text1_split=text1.split(\"\\n\")\n",
    "  text2_split=text2.split(\"\\n\")\n",
    "  text1_cleansed=\"\"\n",
    "  text2_cleansed=\"\"\n",
    "  for item in text1_split:\n",
    "    if item not in text2_split or item.strip()==\"\":\n",
    "      text1_cleansed=text1_cleansed+\"\\n\"+item\n",
    "  for item in text2_split:\n",
    "    if item not in text1_split or item.strip()==\"\":\n",
    "      text2_cleansed=text2_cleansed+\"\\n\"+item\n",
    "  return text1_cleansed.strip(\"\\n\"),text2_cleansed.strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dccf2b83-a1ff-4bc6-8098-01812ce18c9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 6. Functions for Retrieving Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a65d80-c0e6-402a-98b2-209f175e4780",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_index_func(index_dir,index_name,llm=llm,chain_type=\"stuff\"):\n",
    "\n",
    "  ## Create folder structure if not already present\n",
    "  if not os.path.exists(index_dir):\n",
    "      os.makedirs(index_dir)\n",
    "\n",
    "  db_ws.CopyFileFromWorkbench(\"file:\"+index_dir, filename=index_name)\n",
    "\n",
    "  with open(os.path.join(index_dir,index_name), \"rb\") as f:\n",
    "    db_fdd = pickle.load(f)\n",
    "\n",
    "  index_fdd= RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db_fdd.as_retriever())\n",
    "  return index_fdd, db_fdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ed3a222-0064-4cff-b15c-764df2ed9dbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_index_func_params(index_dir,index_name,llm=llm,chain_type=\"stuff\",search_type=\"similarity_score_threshold\",score_threshold=.5,k=4):\n",
    "\n",
    "  ## Create folder structure if not already present\n",
    "  if not os.path.exists(index_dir):\n",
    "      os.makedirs(index_dir)\n",
    "\n",
    "  db_ws.CopyFileFromWorkbench(\"file:\"+index_dir, filename=index_name)\n",
    "\n",
    "  with open(os.path.join(index_dir,index_name), \"rb\") as f:\n",
    "    db_fdd = pickle.load(f)\n",
    "\n",
    "  index_fdd= RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db_fdd.as_retriever(search_type=search_type, search_kwargs={\"score_threshold\": score_threshold, \"k\": k}))\n",
    "  return index_fdd, db_fdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed0d8270-de58-4549-af24-09708d12f7a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Functions for Creating Whoosh Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba7545a-42a4-442c-9f6e-ba71cfba6700",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_documents(zip_filename, zip_dir, blacklist_docs=[]):\n",
    "  # load and chunk word documents\n",
    "  print(\"Parsing\")\n",
    "  documents = parse_fsds(blacklist_docs=blacklist_docs, kg_structure_name=zip_filename, kg_directory=zip_dir)\n",
    "  # warnings.filterwarnings(\"ignore\")\n",
    "  return documents\n",
    "\n",
    "def read_documents_transcripts(doc_path, combine=True, heading=True):\n",
    "  \n",
    "  ## Read the transcripts\n",
    "  transcripts_new = parse_transcripts(doc_path,combine=combine,heading=heading)\n",
    "\n",
    "  ## Quick intermediate step - Write restructured back to JSON (needed for the JSONLoader below)\n",
    "  intermediate_path = \"/tmp/SAP/cleansed_transcript_new_restructured.json\"\n",
    "  with open(intermediate_path, 'w+') as f:\n",
    "      json.dump(transcripts_new, f)\n",
    "\n",
    "  ## Required to retrieve filename as metadata\n",
    "  def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "      metadata[\"transcript_name\"] = record.get(\"filename\")\n",
    "      return metadata\n",
    "\n",
    "  ## Load restructured JSON into langchain (create documents)\n",
    "  loader = JSONLoader(\n",
    "      file_path=intermediate_path,\n",
    "      jq_schema='.[]',\n",
    "      content_key=\"text\",\n",
    "      metadata_func = metadata_func)\n",
    "  texts = loader.load()\n",
    "\n",
    "  return texts\n",
    "\n",
    "def chunk_documents(documents, chunk_size=5000, chunk_overlap=1000, heading=True):\n",
    "  print(\"Chunking\")\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap) #TokenTextSplitter\n",
    "  texts = text_splitter.split_documents(documents)\n",
    "  if heading:\n",
    "    for index,chunk in enumerate(texts):\n",
    "      source_filepath=chunk.metadata['source']\n",
    "      source_filename=source_filepath[:source_filepath.rfind(\".\")].split('/')[-1]\n",
    "      texts[index].page_content='Document Name: '+source_filename+'\\n\\n'+chunk.page_content\n",
    "  return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c641459-7233-4604-9e98-f6fa24194727",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_embedded_index(documents,index_dir,index_name,embedding_type='OpenAI',index_type='FAISS', save_to_workbench=True):\n",
    "\n",
    "  print(\"Creating embeddings\")\n",
    "  # create embeddings\n",
    "  if embedding_type.replace(' ','').lower()=='huggingface':\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "  elif embedding_type.replace(' ','').lower()=='openai':\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "      deployment=deployment_id_ada,\n",
    "      model=\"text-embedding-ada-002\",\n",
    "      openai_api_key=openai_api_key,\n",
    "      chunk_size=1\n",
    "    )\n",
    "\n",
    "  # create vector store\n",
    "  print(\"Embedding text\")\n",
    "  if index_type.replace(' ','').lower()=='faiss':\n",
    "    db = FAISS.from_documents(documents, embeddings)\n",
    "  # elif index_type.replace(' ','').lower()=='chroma':\n",
    "  #   db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "  if not save_to_workbench or index_dir==None or index_name==None :\n",
    "    return db\n",
    "  \n",
    "  else:\n",
    "    print(\"Saving index to file\")\n",
    "    ## Save in local directory\n",
    "    if not os.path.exists(index_dir):\n",
    "      os.makedirs(index_dir)\n",
    "\n",
    "    with open(os.path.join(index_dir,index_name), \"wb\") as f:\n",
    "      pickle.dump(db,f)\n",
    "    db_ws.CopyFileToWorkbench(filename=index_name, DatabricksFolder=\"file:\"+index_dir)\n",
    "    print('Index'+index_name+' pickled and copied to Workbench')\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9465e39-888d-43a5-81b3-b6b0feb26b7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DocumentIndexWhoosh():\n",
    "  def __init__(self, documents, index_path_out, stem=True):\n",
    "\n",
    "    ## Create schema for indexing\n",
    "    stem_ana = StemmingAnalyzer()\n",
    "    schema = Schema(title=ID(stored=True), content=TEXT(stored = True), content_stem=TEXT(analyzer=stem_ana))\n",
    "    \n",
    "    ## Create index\n",
    "    if not os.path.exists(index_path_out):\n",
    "      os.mkdir(index_path_out)\n",
    "    else:\n",
    "      shutil.rmtree(index_path_out)\n",
    "      os.mkdir(index_path_out)\n",
    "    ix = create_in(index_path_out, schema)\n",
    "    writer = ix.writer()\n",
    "\n",
    "    document_dict = []\n",
    "    for document in documents:\n",
    "      document_dict.append({\"text\": document.page_content, \"filename\": document.metadata[\"source\"]})\n",
    "\n",
    "    ## Fill index with documents\n",
    "    for document in document_dict:\n",
    "      writer.add_document(title=document[\"filename\"], content=document[\"text\"], content_stem=document[\"text\"])\n",
    "    writer.commit()\n",
    "    self._index = ix\n",
    "\n",
    "    ## Configure parser to parse search queries\n",
    "    search_field = \"content\"\n",
    "    if stem: search_field = \"content_stem\"\n",
    "    parser = QueryParser(search_field, self._index.schema, group=OrGroup.factory(0.9))\n",
    "    # using OrGroup.factory() with a high value adds a bonus if multiple query words are retrieved (see https://whoosh.readthedocs.io/en/latest/parsing.html)\n",
    "    parser.add_plugin(FuzzyTermPlugin())\n",
    "    self._parser = parser\n",
    "  \n",
    "  def search(self, query, k=5, stem=True, fuzzy_dist=0):\n",
    "    if stem:\n",
    "      ana = StemmingAnalyzer()\n",
    "      query = \" \".join([token.text + \"~\" + str(fuzzy_dist) for token in ana(query)]) #'~1' says that each word can be one character off (fuzzy search)\n",
    "    else:\n",
    "      query = \" \".join([word + \"~\" + str(fuzzy_dist) for word in query.split(\" \")])\n",
    "    try:\n",
    "      # with self._index.searcher() as searcher:\n",
    "      searcher = self._index.searcher()\n",
    "      parsed_query = self._parser.parse(query)\n",
    "      res = searcher.search(parsed_query, terms=True, limit=k)\n",
    "      # Return only the content of the best result (to be able to close the searcher().\n",
    "      # If returning res in full, we'd get an \"ReaderClosed\" exception.)\n",
    "      return res\n",
    "    except Exception as err:\n",
    "      print(\"Error in searching whoosh index. Returning empty string (''). Traceback: \" + str(err))\n",
    "      return \"\"\n",
    "    \n",
    "  def search_and_summarize(self, search_term, k=3, stem=True, fuzzy_dist=0):\n",
    "    res = \"None\"\n",
    "    try:\n",
    "    \n",
    "      # Get initial results from whoosh search\n",
    "      whoosh_results = self.search(search_term, stem=stem, fuzzy_dist=fuzzy_dist)\n",
    "\n",
    "      # avoid index errors (e.g. when we specify top 5, but whoosh only retrieves 3 docs)\n",
    "      if len(whoosh_results) < k:\n",
    "        k = len(whoosh_results)\n",
    "      \n",
    "      # concatenate top k results from the whoosh search in plain text\n",
    "      top_k_docs = [entry[\"content\"] for entry in whoosh_results[:k]]\n",
    "\n",
    "      # have GPT summarize the whoosh-retrieved documents (with a focus on the search keywords)\n",
    "      summary_prompt = f\"\"\"\n",
    "      What information from the documents below can be leveraged to build the following requirement '{search_term}'. Elaborate as much as possible. Documents:\n",
    "      '''{top_k_docs}'''\n",
    "      \"\"\"\n",
    "      res = gpt4(summary_prompt, large=True, max_tokens=10000, tries=3, temperature=.2)\n",
    "\n",
    "    except Exception as err:\n",
    "      log(\"[Whoosh Retrieval] Error in searching documents using whoosh & GPT. Returning 'None'. Traceback: \")\n",
    "      print(err)\n",
    "\n",
    "    finally:\n",
    "      return res\n",
    "    \n",
    "## Could also be loaded via ix = index.open_dir(\"indexdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1985b6b4-4ad7-471f-a0e3-711c4ebfa12b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Functions for creating a combined Whoosh & FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a7a7c25-4468-4999-9cd6-88c90774ffd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CombinedIndex():\n",
    "\n",
    "  def __init__(self, faiss_index, whoosh_index):\n",
    "    self._faiss_index = faiss_index\n",
    "    self._whoosh_index = whoosh_index\n",
    "    self._validate_chunk_equality()\n",
    "\n",
    "  def _validate_chunk_equality(self):\n",
    "    ## Check if individual whoosh and faiss chunks are identical\n",
    "    chunks_whoosh = []\n",
    "    chunks_faiss = []\n",
    "    for doc in self._whoosh_index._index.searcher().documents():\n",
    "      chunks_whoosh.append(doc[\"content\"])\n",
    "    for doc_id in self._faiss_index.docstore._dict.keys():\n",
    "      chunks_faiss.append(self._faiss_index.docstore._dict[doc_id].page_content)\n",
    "    if not len(chunks_whoosh) == len(chunks_faiss): raise ValueError(\"Whoosh and FAISS have do not have the same amount of chunks\")\n",
    "    for i in range(len(chunks_faiss)):\n",
    "      if not chunks_whoosh[i] == chunks_faiss[i]: raise ValueError(\"Whoosh and FAISS chunk text different for chunk number \" + str(i))\n",
    "\n",
    "  def search_both_indexes(self, query, k=5, verbose=False):\n",
    "    whoosh_results_raw = self._whoosh_index.search(query, k=k)\n",
    "    k_whoosh = min(k, len(whoosh_results_raw)) # avoid index errors (e.g. when we specify top 5, but whoosh only retrieves 3 docs)\n",
    "    if verbose and k_whoosh < k: log(f\"[Combined Index Lookup] Whoosh only retrieved k={k_whoosh} results. You specified k={k}.\")\n",
    "    whoosh_results = [entry[\"content\"] for entry in whoosh_results_raw[:k_whoosh]]\n",
    "\n",
    "    faiss_results_raw = self._faiss_index.similarity_search(query, k)\n",
    "    k_faiss= min(k, len(faiss_results_raw)) # avoid index errors (e.g. when we specify top 5, but whoosh only retrieves 3 docs)\n",
    "    if verbose and k_faiss < k: log(f\"[Combined Index Lookup] FAISS only retrieved k={k_faiss} results. You specified k={k}.\")\n",
    "    faiss_results = [entry.page_content for entry in faiss_results_raw[:k_faiss]]\n",
    "\n",
    "    combined_results = [x for x in itertools_chain.from_iterable(zip_longest(whoosh_results,faiss_results)) if x]\n",
    "    # complicated, but needed to preserve scoring order. essentially, this constructes an intertwined list aka [whoosh_1, faiss_1, whoosh_2, faiss_2, ...]\n",
    "    # instead of a naive append ([whoosh_1, whoosh_2, ... , faiss_1, faiss_2] or vice versa).\n",
    "    # note that this prioritizes whoosh over faiss for the first result - change to zip_longest(faiss_results, whoosh_results) to reverse\n",
    "\n",
    "    duplicates = set()\n",
    "    unique_results = []\n",
    "    for x in combined_results:\n",
    "      if x not in unique_results:\n",
    "        unique_results.append(x)\n",
    "      else:\n",
    "        duplicates.add(x)\n",
    "    if verbose and len(duplicates) > 0: log(f\"[Combined Index Lookup] FAISS and Whoosh returned partially same results. Removed {len(duplicates)} duplicates.\")\n",
    "\n",
    "    return unique_results\n",
    "  \n",
    "  def search_and_summarize(self, query, k=5, verbose=False):\n",
    "    res = \"None\"\n",
    "    try:\n",
    "    \n",
    "      # Get initial results from whoosh search\n",
    "      results = self.search_both_indexes(query=query, k=k, verbose=verbose)\n",
    "\n",
    "      # have GPT summarize the whoosh-retrieved documents (with a focus on the search keywords)\n",
    "      summary_prompt = f\"\"\"\n",
    "      For each of the documents below, list all detailed granular information that can be leveraged to build the requirement '{query}'. Response should be each document name followed by relevant info. Elaborate as much as possible. Documents:\n",
    "      '''{results}'''\n",
    "      \"\"\"\n",
    "      res = gpt4(summary_prompt, large=True, max_tokens=5000, tries=3, temperature=.2)\n",
    "\n",
    "    except Exception as err:\n",
    "      log(\"[Combined Index Lookup] Error in searching meeting transcripts using the combined Whoosh & FAISS index. Returning 'None'. Traceback: \")\n",
    "      print(err)\n",
    "\n",
    "    finally:\n",
    "      return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "920f54f0-1ba5-48f5-8323-1f4b702e50a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 7. Functions for Retrieving Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7931f54c-d2ab-45e5-a76b-cd234a830c28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_knowledge_base(kb_dir, kb_name):\n",
    "\n",
    "  ## Create folder structure if not already present\n",
    "  if not os.path.exists(kb_dir):\n",
    "      os.makedirs(kb_dir)\n",
    "\n",
    "  db_ws.CopyFileFromWorkbench(\"file:\"+kb_dir, filename=kb_name)\n",
    "\n",
    "  with open(os.path.join(kb_dir, kb_name), \"r\") as f:\n",
    "    kb = json.load(f)\n",
    "\n",
    "  return kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "462cfd51-ee74-41de-86f0-fe4dadff50b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 8. Functions for Parsing Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc10fa05-5352-4506-9f36-bee8ee632c40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def parse_transcripts(doc_path,combine=True,heading=False):\n",
    "  ## Load JSON into memory\n",
    "  # json.load() directly doesn't work for some reason\n",
    "  with open(doc_path, \"r\") as f:\n",
    "    content = f.read()\n",
    "  transcripts = json.loads(content)\n",
    "\n",
    "  ## Restructure the JSON for further processing\n",
    "  transcripts_new = []\n",
    "  if combine:\n",
    "    for transcript_name in transcripts:\n",
    "      if heading:\n",
    "        filename_heading='Document Name: '+transcript_name+'\\n\\n'\n",
    "      else:\n",
    "        filename_heading=''\n",
    "      full_text = \"\\n\\n\".join(transcripts[transcript_name])\n",
    "      transcripts_new.append({\"filename\" : transcript_name, \"text\" : filename_heading+full_text})\n",
    "  else:\n",
    "    for transcript_name in transcripts:\n",
    "      if heading:\n",
    "        filename_heading='Document Name: '+transcript_name+'\\n\\n'\n",
    "      else:\n",
    "        filename_heading=''\n",
    "      for index,text in enumerate(transcripts[transcript_name]):\n",
    "        transcripts_new.append({\"filename\" : transcript_name+' chunk '+str(index+1), \"text\" : filename_heading+text})\n",
    "  \n",
    "  return transcripts_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7226dab-4d0e-49fa-9dfd-f1e58b741396",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_transcripts(transcript_path, txt_transcripts):\n",
    "  with open(os.path.join(transcript_path,txt_transcripts), \"r\") as f:\n",
    "    content = f.read()\n",
    "    transcripts = json.loads(content)\n",
    "\n",
    "  transcripts_new = []\n",
    "  for transcript_name in transcripts:\n",
    "    full_text = \"\\n\\n\".join(transcripts[transcript_name])\n",
    "    transcripts_new.append({\"filename\" : transcript_name, \"text\" : full_text})\n",
    "  return transcripts_new\n",
    "\n",
    "\n",
    "def requirements_matching(df_requirements, transcripts_new, requirements_chunk_size=5000):\n",
    "  \n",
    "  ## first break out the transcripts text into a dictionary scructure\n",
    "\n",
    "  ## write the prompt\n",
    "  prompt =  \"\"\"###Start of listing of requirement names\n",
    "  {requirement_names}\n",
    "  ###End of listing of requirement names\n",
    " \n",
    "  ###Start of transcript\n",
    "  {transcript_chunk}\n",
    "  ###End of transcript\n",
    " \n",
    "  From the above requirement names and transcript, list the requirement names that may have some relation to portions of the transcript content using the JSON example template below, value of 'requirement name' element must be selected from listing of requirement names above (don't change the original requirement name value, don't need to return requirement names that did not find any relevant transcript content), the 'portions of transcript content' elements should be populated by information from the transcript section above only (never populate from listing of requirement names), only return the JSON list as your response, data type of 'portions of transcript content' element must be text, ensure character escape is implemented where applicable so that json.loads() can read your response (always escape '\\' with '/\\' instead of '\\\\'):\n",
    " \n",
    "  [{\"requirement name\": \"ABC\", \"portions of transcript content\": \"UVW\"},\n",
    "  {\"requirement name\": \"DEF\", \"portions of transcript content\": \"XYZ\"}]\"\"\"\n",
    "\n",
    "\n",
    "  projectbackground = \"This project is an SAP S4 transformation focusing on finance, using SAP's fit to standard approach.\"\n",
    "\n",
    "  systemprompt = \"You specialize in mapping portions of transcripts to requirement names for projects. \" + projectbackground\n",
    "\n",
    "  list_requirements = df_requirements['Requirement Cleansed'].tolist()\n",
    "  token_count_requirement=count_tokens(str(list_requirements))\n",
    "\n",
    "  if token_count_requirement > requirements_chunk_size:\n",
    "      n = ceil(token_count_requirement / requirements_chunk_size)\n",
    "      print(f\"Warning: Token count for requirement list is {token_count_requirement} and thus higher than the specified max ({requirements_chunk_size}). Splitting requirement list into {n} chunks.\")\n",
    "      chunks = list(slice_list(list_requirements, n))\n",
    "      requirement_chunks = {\"chunks\": chunks}\n",
    "  else:\n",
    "      n=1\n",
    "      requirement_chunks = {\"chunks\": [list_requirements]}\n",
    "\n",
    "  ## for testing shorter list of transcript chunks\n",
    "  # list_short = transcripts_new[:3]\n",
    "\n",
    "  # ## loop through\n",
    "  for index,transcript in req_trans_control_table[req_trans_control_table['status']==''].iterrows():\n",
    "    transcript_response=[]\n",
    "    for chunk_index,chunk in enumerate(requirement_chunks[\"chunks\"]):\n",
    "      chunk_requirements_list='\\n\\n'.join(chunk)\n",
    "      current_prompt = prompt.replace(\"{requirement_names}\", chunk_requirements_list).replace(\"{transcript_chunk}\", str(transcript['text']))\n",
    "      #log(current_prompt) #DEBUG\n",
    "      # response = gpt4(prompt=current_prompt, context=systemprompt, temperature=temperature, max_tokens=15000, large=True, tries=3)\n",
    "      res = gpt35(prompt=current_prompt, temperature=temperature, max_tokens=8000, tries=3)\n",
    "      # log(response) #DEBUG\n",
    "      req_trans_control_table.loc[index,'response']=response\n",
    "      # if 1==1:\n",
    "      try:\n",
    "        response_json=json.loads(response)\n",
    "        transcript_response=transcript_response+response_json\n",
    "        if chunk_index+1==n:\n",
    "          req_trans_control_table.loc[index,'response']=json.dumps(transcript_response)\n",
    "          req_trans_control_table.loc[index,'status']='complete'\n",
    "        print(\"transcript \"+str(index+1)+\" chunk \"+str(chunk_index+1)+\" processed...\")\n",
    "      except:\n",
    "        print(f\"\"\"JSON conversion failed:\n",
    "              {response}\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "  ## for testing single prompt\n",
    "\n",
    "  # log(\"testing with requirement name\")\n",
    "\n",
    "  # current_prompt = prompt.replace(\"{requirement_names}\", str(df_requirements['RICEFW Name'].tolist())).replace(\"{transcript_chunk}\", str(transcripts_new[0]))\n",
    "\n",
    "  # response = gpt4(prompt=current_prompt, context=systemprompt, temperature=temperature, max_tokens=20000, large=True, tries=3)\n",
    "\n",
    "\n",
    "  # return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7464e8a9-0a0a-453b-8979-688f264f6379",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_requirements_matching(path, filename):\n",
    "  \n",
    "  ## Create folder structure if not already present\n",
    "  if not os.path.exists(path):\n",
    "      os.makedirs(path)\n",
    "\n",
    "  db_ws.CopyFileFromWorkbench(\"file:\"+path, filename=filename)\n",
    "\n",
    "  df = pd.read_excel(os.path.join(path,filename))\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e41f791b-96d7-4307-98c0-430b4ac866bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Functions for parsing sample FSDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d15ff20-09d9-42c2-be44-8d83fe804440",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def parse_fsds(blacklist_docs=[],kg_structure_name='FSD',kg_directory='/SAP_KG/'):\n",
    "  ## Define location of Indexing files & respective folder names\n",
    "  kg_structure = [kg_structure_name]\n",
    "  ## Load all files and convert them to usable format\n",
    "  create_directories(kg_structure)\n",
    "  unzip_and_convert(kg_structure,kg_directory,black_list_docs=blacklist_docs,convert_verbose=True)\n",
    "  fsd_documents = DirectoryLoader(os.path.join(kg_directory,kg_structure_name)).load()\n",
    "  return fsd_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94eb9c85-242f-40ee-bdb9-454f28518fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 9. Functions for Parsing Templates (generate placeholder dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b6e53c4-91aa-461a-b409-4c76f9ff47ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_templates(input_folder, max_tokens_per_chunk=800):\n",
    "  templatedict={}\n",
    "  #Read Template Docs\n",
    "  for row in df_mapping['Template Name'].drop_duplicates():\n",
    "    if len(pathlib.Path(row).suffix)>0:\n",
    "      templatefilename=row\n",
    "    else:\n",
    "      templatefilename=row+'.docx'\n",
    "    try:\n",
    "      print(f\"\\nParsing '{templatefilename}'\")\n",
    "      db_ws.CopyFileFromWorkbench(DatabricksFolder=\"file:\"+input_folder, filename=templatefilename)\n",
    "      doc, placeholders, originalpositions = read_document(input_folder+templatefilename,max_tokens_per_chunk=max_tokens_per_chunk)\n",
    "    except:\n",
    "      print('Template ' + templatefilename+' not found on Workbench! Please verify template is uploaded and template name is correct in '+filename_mapping)\n",
    "    templatedict[row]={'placeholders':placeholders,'docs':doc,'originalpositions':originalpositions}\n",
    "  \n",
    "  return templatedict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f4eace3-ac73-4038-8d0a-10fa8354957c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_transcript_match(dirty_df):\n",
    " return dirty_df['response'].DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cb93d6a-d7b4-4909-9532-166957b990cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 10. Decorators and Control Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a04eeb7-1c9c-49de-9ee3-1c2dc899ecf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def keep_trying(func):\n",
    "  def wrapper(*args, **kwargs):\n",
    "    log(\"[CONTROL] INITIALIZING - Running main function body in 'keep_trying' mode.\", color=\"blue\")\n",
    "    while True:\n",
    "      try:\n",
    "        exit = func(*args, **kwargs)\n",
    "        if exit:\n",
    "          log(\"[CONTROL] SUCCESS - Main function body terminated as per exit criteria.\", color=\"green\")\n",
    "          break\n",
    "        else:\n",
    "          log(\"[CONTROL] [RERUN] Rerunning indefinitely as 'keep_trying' is on. Rerunning now.\", color=\"blue\")\n",
    "      except Exception as err:\n",
    "        log(\"[CONTROL] [CRITICAL ERROR] Error in main function body. Traceback: \" + str(err), color=\"red\")\n",
    "        log(\"[CONTROL] [RERUN] Rerunning indefinitely as 'keep_trying' is on. Rerunning now.\", color=\"blue\")\n",
    "        continue\n",
    "  return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8592c5d3-8cb9-4853-aacd-717cc3feb737",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####  11. Functions for Validation of provided documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcb23c6c-7312-4b32-beea-63fbb2b9c564",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def validate_docs(source_filepath, generated_filepath, verbose= False):\n",
    "\n",
    "  source_filename =  source_filepath\n",
    "  generate_filname = generated_filepath\n",
    "  local_save_folder = \"/tmp/SAP/\"\n",
    "  #why is file path wrong \n",
    "  Folder1= Folder1=\"/tmp/Input_SAP/\"\n",
    "\n",
    "  # dbutils.fs.mkdirs(local_save_folder) #comment\n",
    "\n",
    "  source_docx =db_ws.CopyFileFromWorkbench(DatabricksFolder=local_save_folder,filename=source_filename)\n",
    "  generate_docx =db_ws.CopyFileFromWorkbench(DatabricksFolder=local_save_folder,filename=generate_filname)\n",
    "\n",
    "  source_doc = convert(source_filename,for_validation = True)\n",
    "  generated_doc = convert(generate_filname, for_validation = True)\n",
    "\n",
    "  header_list = create_heading_list(Folder1 + 'OGE_Functional Specification Document_ENH Template__Approved.docx')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  model = sentence_transformers.SentenceTransformer(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "  source_df = split_text_to_df(source_doc, header_list)\n",
    "  generated_df = split_text_to_df(generated_doc, header_list)\n",
    "\n",
    "  joined_df = source_df.set_index('Headers').join(generated_df.set_index('Headers'), lsuffix=\"_source\", rsuffix=\"_generated\", how= \"inner\")\n",
    "  joined_df['cos_sim'] = ''\n",
    "  if verbose == True:\n",
    "    joined_df['source row processed'] = ''\n",
    "    joined_df['gen row processed'] = ''\n",
    "    joined_df['source row cleansed']=''\n",
    "    joined_df['gen row cleansed']=''\n",
    "\n",
    "  for index, row in joined_df.iterrows():\n",
    "    ## we want to clean the inputs first by taking out any parts that do not match\n",
    "    prefix = os.path.commonprefix([row['Sections_source'],row['Sections_generated']])\n",
    "\n",
    "  \n",
    "\n",
    "    if prefix == '':\n",
    "      source_row_processed = row['Sections_source']\n",
    "      gen_row_processed = row['Sections_generated']\n",
    "      # joined_df.drop(index = row['Headers'])\n",
    "    else:\n",
    "      source_row_processed = row['Sections_source'][len(prefix):len(row['Sections_source'])]\n",
    "      gen_row_processed = row['Sections_generated'][len(prefix):len(row['Sections_generated'])]\n",
    "      if verbose == True:\n",
    "        row['source row processed'] = source_row_processed\n",
    "        row['gen row processed'] = gen_row_processed\n",
    "\n",
    "    source_row_cleansed,gen_row_cleansed=remove_dups_from_text(source_row_processed,gen_row_processed)\n",
    "    if verbose == True:\n",
    "      row['source row cleansed'] = source_row_cleansed\n",
    "      row['gen row cleansed'] = gen_row_cleansed\n",
    "\n",
    "    ## then we do the \n",
    "    source_embedding= model.encode(source_row_cleansed, convert_to_tensor=True)\n",
    "    gen_embedding = model.encode(gen_row_cleansed, convert_to_tensor=True)\n",
    "    row['cos_sim']= util.pytorch_cos_sim(source_embedding, gen_embedding).tolist()[0][0]\n",
    "\n",
    "    # cleaned_df = joined_df[joined_df['cos_sim']<.999]\n",
    "    \n",
    "\n",
    "  return joined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "618a5801-317f-4b4e-93e7-85f5a48ff4e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cleanup_dupes(input_df):\n",
    "  cleaned_df = input_df[input_df['cos_sim']<.999]\n",
    "  return cleaned_df "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "SAP_FDD_Env_and_Utils",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
